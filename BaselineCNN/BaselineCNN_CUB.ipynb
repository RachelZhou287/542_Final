{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6a5221-7a25-4913-b463-8fc1be878742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38afb1ab-a1f5-41a8-aa51-bb4fdfb4c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_ROOT = \"CUB_200_2011\"\n",
    "NUM_CLASSES = 200\n",
    "INIT_LR = 1e-4\n",
    "MID_LR = 1e-5\n",
    "FINAL_LR = 1e-6\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "STEP_SIZE = 7\n",
    "EPOCHS_STAGE1 = 10\n",
    "EPOCHS_STAGE2 = 20\n",
    "EPOCHS_STAGE3 = 70\n",
    "SEED = 542\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecfc580-fdd3-4717-84f9-ea312fa52053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        img_txt = os.path.join(root, \"images.txt\")\n",
    "        label_txt = os.path.join(root, \"image_class_labels.txt\")\n",
    "        split_txt = os.path.join(root, \"train_test_split.txt\")\n",
    "\n",
    "        with open(img_txt) as f:\n",
    "            imgs = [x.strip().split(\" \") for x in f.readlines()]\n",
    "        with open(label_txt) as f:\n",
    "            labels = [int(x.strip().split(\" \")[1]) - 1 for x in f.readlines()]\n",
    "        with open(split_txt) as f:\n",
    "            split = [int(x.strip().split(\" \")[1]) for x in f.readlines()]\n",
    "\n",
    "        self.samples = []\n",
    "        for (img_id, img_path), label, is_train in zip(imgs, labels, split):\n",
    "            if (train and is_train == 1) or (not train and is_train == 0):\n",
    "                self.samples.append((os.path.join(root, \"images\", img_path), label))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    CUBDataset(DATA_ROOT, True, train_transforms),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "    pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    CUBDataset(DATA_ROOT, False, test_transforms),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n",
    "    pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
    ")\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return 100 * correct / total, total_loss / total\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, total = 0.0, 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        # MixUp\n",
    "        mixed_imgs, y_a, y_b, lam = mixup_data(images, labels, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_imgs)\n",
    "        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "\n",
    "        # correctly accumulate *sample-level* loss\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return running_loss / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48549210-42e6-454e-b3e5-be175d1b748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES,pretrained=True):\n",
    "        super().__init__()\n",
    "        backbone = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        # Extract features\n",
    "        self.features = backbone.features\n",
    "        \n",
    "        # Extract classifier but replace last FC\n",
    "        classifier=list(backbone.classifier.children())\n",
    "        in_features=classifier[-1].in_features\n",
    "        classifier[-1]=nn.Linear(in_features,num_classes)\n",
    "        self.classifier=nn.Sequential(*classifier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x=torch.flatten(x,1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968721e4-bcbc-4b9b-b371-b8723ac449ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_3stage(model_name, model):\n",
    "    print(\"=== Training ===\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### Stage 1: Train classifier only \n",
    "    for p in model.features.parameters(): \n",
    "        p.requires_grad = False\n",
    "    for p in model.classifier.parameters(): \n",
    "        p.requires_grad = True\n",
    "\n",
    "    print(\"--- Stage 1 ---\")\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=INIT_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=0.1)\n",
    "\n",
    "    stage1_hist = []\n",
    "    for epoch in range(EPOCHS_STAGE1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "\n",
    "        stage1_hist.append((epoch+1, tr_loss, te_loss, te_acc))\n",
    "\n",
    "        print(f\"[S1-Epoch {epoch+1:02d}] \"\n",
    "              f\"Train Loss={tr_loss:.4f} | Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}%\")\n",
    "\n",
    "    ### Stage 2: Fine-tune entire network \n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    print(\"--- Stage 2---\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=MID_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=EPOCHS_STAGE2, gamma=0.1)\n",
    "\n",
    "    stage2_hist = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS_STAGE2):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "\n",
    "        stage2_hist.append((epoch+1, tr_loss, te_loss, te_acc))\n",
    "\n",
    "        if te_acc > best_acc:\n",
    "            best_acc = te_acc\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pt\")\n",
    "\n",
    "        print(f\"[S2-Epoch {epoch+1:03d}] \"\n",
    "              f\"Train Loss={tr_loss:.4f} | Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}%\")\n",
    "\n",
    "    ### Stage 3: Low-LR refinement \n",
    "    print(\"--- Stage 3---\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=FINAL_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=EPOCHS_STAGE3, gamma=0.1)\n",
    "\n",
    "    stage3_hist = []\n",
    "\n",
    "    for epoch in range(EPOCHS_STAGE3):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "\n",
    "        stage3_hist.append((epoch+1, tr_loss, te_loss, te_acc))\n",
    "\n",
    "        print(f\"[S3-Epoch {epoch+1:03d}] \"\n",
    "              f\"Train Loss={tr_loss:.4f} | Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    return stage1_hist, stage2_hist, stage3_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39115bd8-d46d-4e4e-ac87-2f8030a04b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n",
      "--- Stage 1 ---\n",
      "[S1-Epoch 01] Train Loss=4.6580 | Test Loss=3.0247 | Test Acc=26.72%\n",
      "[S1-Epoch 02] Train Loss=3.4507 | Test Loss=2.3118 | Test Acc=41.34%\n",
      "[S1-Epoch 03] Train Loss=3.0850 | Test Loss=1.9696 | Test Acc=48.41%\n",
      "[S1-Epoch 04] Train Loss=2.8945 | Test Loss=1.7919 | Test Acc=51.98%\n",
      "[S1-Epoch 05] Train Loss=2.7037 | Test Loss=1.8005 | Test Acc=51.92%\n",
      "[S1-Epoch 06] Train Loss=2.4097 | Test Loss=1.6422 | Test Acc=55.25%\n",
      "[S1-Epoch 07] Train Loss=2.4397 | Test Loss=1.5785 | Test Acc=56.18%\n",
      "[S1-Epoch 08] Train Loss=2.1281 | Test Loss=1.4500 | Test Acc=60.01%\n",
      "[S1-Epoch 09] Train Loss=2.1225 | Test Loss=1.4252 | Test Acc=60.46%\n",
      "[S1-Epoch 10] Train Loss=2.0719 | Test Loss=1.4157 | Test Acc=61.06%\n",
      "--- Stage 2---\n",
      "[S2-Epoch 001] Train Loss=2.1555 | Test Loss=1.3043 | Test Acc=63.63%\n",
      "[S2-Epoch 002] Train Loss=1.9879 | Test Loss=1.2621 | Test Acc=65.19%\n",
      "[S2-Epoch 003] Train Loss=1.8906 | Test Loss=1.2125 | Test Acc=66.60%\n",
      "[S2-Epoch 004] Train Loss=1.8294 | Test Loss=1.1624 | Test Acc=67.09%\n",
      "[S2-Epoch 005] Train Loss=1.6491 | Test Loss=1.1382 | Test Acc=68.10%\n",
      "[S2-Epoch 006] Train Loss=1.7910 | Test Loss=1.1297 | Test Acc=68.86%\n",
      "[S2-Epoch 007] Train Loss=1.7136 | Test Loss=1.1138 | Test Acc=69.09%\n",
      "[S2-Epoch 008] Train Loss=1.6116 | Test Loss=1.0981 | Test Acc=69.18%\n",
      "[S2-Epoch 009] Train Loss=1.5036 | Test Loss=1.0645 | Test Acc=70.07%\n",
      "[S2-Epoch 010] Train Loss=1.5967 | Test Loss=1.0483 | Test Acc=70.61%\n",
      "[S2-Epoch 011] Train Loss=1.4737 | Test Loss=1.0697 | Test Acc=70.19%\n",
      "[S2-Epoch 012] Train Loss=1.4257 | Test Loss=1.0433 | Test Acc=70.49%\n",
      "[S2-Epoch 013] Train Loss=1.4394 | Test Loss=1.0247 | Test Acc=71.56%\n",
      "[S2-Epoch 014] Train Loss=1.4576 | Test Loss=1.0449 | Test Acc=71.75%\n",
      "[S2-Epoch 015] Train Loss=1.4921 | Test Loss=1.0305 | Test Acc=71.94%\n",
      "[S2-Epoch 016] Train Loss=1.3728 | Test Loss=0.9999 | Test Acc=72.33%\n",
      "[S2-Epoch 017] Train Loss=1.5047 | Test Loss=1.0297 | Test Acc=71.87%\n",
      "[S2-Epoch 018] Train Loss=1.4602 | Test Loss=1.0033 | Test Acc=72.37%\n",
      "[S2-Epoch 019] Train Loss=1.4063 | Test Loss=1.0282 | Test Acc=72.28%\n",
      "[S2-Epoch 020] Train Loss=1.2265 | Test Loss=0.9785 | Test Acc=72.44%\n",
      "--- Stage 3---\n",
      "[S3-Epoch 001] Train Loss=1.3417 | Test Loss=1.0371 | Test Acc=72.32%\n",
      "[S3-Epoch 002] Train Loss=1.2489 | Test Loss=0.9583 | Test Acc=72.70%\n",
      "[S3-Epoch 003] Train Loss=1.3802 | Test Loss=0.9904 | Test Acc=72.87%\n",
      "[S3-Epoch 004] Train Loss=1.4265 | Test Loss=0.9772 | Test Acc=72.76%\n",
      "[S3-Epoch 005] Train Loss=1.2082 | Test Loss=0.9721 | Test Acc=72.83%\n",
      "[S3-Epoch 006] Train Loss=1.1954 | Test Loss=0.9897 | Test Acc=72.83%\n",
      "[S3-Epoch 007] Train Loss=1.3616 | Test Loss=0.9837 | Test Acc=72.95%\n",
      "[S3-Epoch 008] Train Loss=1.2220 | Test Loss=0.9699 | Test Acc=72.95%\n",
      "[S3-Epoch 009] Train Loss=1.1781 | Test Loss=0.9535 | Test Acc=73.18%\n",
      "[S3-Epoch 010] Train Loss=1.5341 | Test Loss=0.9802 | Test Acc=73.14%\n",
      "[S3-Epoch 011] Train Loss=1.2276 | Test Loss=0.9886 | Test Acc=72.99%\n",
      "[S3-Epoch 012] Train Loss=1.3296 | Test Loss=0.9693 | Test Acc=73.04%\n",
      "[S3-Epoch 013] Train Loss=1.1394 | Test Loss=0.9450 | Test Acc=73.39%\n",
      "[S3-Epoch 014] Train Loss=1.3284 | Test Loss=1.0155 | Test Acc=72.89%\n",
      "[S3-Epoch 015] Train Loss=1.1938 | Test Loss=0.9746 | Test Acc=73.18%\n",
      "[S3-Epoch 016] Train Loss=1.2485 | Test Loss=0.9876 | Test Acc=73.14%\n",
      "[S3-Epoch 017] Train Loss=1.3597 | Test Loss=0.9722 | Test Acc=73.13%\n",
      "[S3-Epoch 018] Train Loss=1.2672 | Test Loss=0.9646 | Test Acc=73.18%\n",
      "[S3-Epoch 019] Train Loss=1.2310 | Test Loss=0.9857 | Test Acc=73.16%\n",
      "[S3-Epoch 020] Train Loss=1.2843 | Test Loss=0.9762 | Test Acc=73.16%\n",
      "[S3-Epoch 021] Train Loss=1.2243 | Test Loss=0.9941 | Test Acc=73.30%\n",
      "[S3-Epoch 022] Train Loss=1.2856 | Test Loss=0.9933 | Test Acc=73.11%\n",
      "[S3-Epoch 023] Train Loss=1.2971 | Test Loss=0.9635 | Test Acc=73.52%\n",
      "[S3-Epoch 024] Train Loss=1.1577 | Test Loss=0.9757 | Test Acc=73.33%\n",
      "[S3-Epoch 025] Train Loss=1.2739 | Test Loss=0.9724 | Test Acc=73.35%\n",
      "[S3-Epoch 026] Train Loss=1.2154 | Test Loss=0.9412 | Test Acc=73.47%\n",
      "[S3-Epoch 027] Train Loss=1.3194 | Test Loss=0.9519 | Test Acc=73.61%\n",
      "[S3-Epoch 028] Train Loss=1.2842 | Test Loss=0.9599 | Test Acc=73.58%\n",
      "[S3-Epoch 029] Train Loss=1.2406 | Test Loss=0.9632 | Test Acc=73.56%\n",
      "[S3-Epoch 030] Train Loss=1.3046 | Test Loss=0.9955 | Test Acc=73.52%\n",
      "[S3-Epoch 031] Train Loss=1.2521 | Test Loss=0.9567 | Test Acc=73.66%\n",
      "[S3-Epoch 032] Train Loss=1.3452 | Test Loss=0.9732 | Test Acc=73.30%\n",
      "[S3-Epoch 033] Train Loss=1.3945 | Test Loss=0.9651 | Test Acc=73.71%\n",
      "[S3-Epoch 034] Train Loss=1.3087 | Test Loss=0.9366 | Test Acc=73.52%\n",
      "[S3-Epoch 035] Train Loss=1.2527 | Test Loss=0.9601 | Test Acc=73.54%\n",
      "[S3-Epoch 036] Train Loss=1.1866 | Test Loss=0.9454 | Test Acc=73.63%\n",
      "[S3-Epoch 037] Train Loss=1.2332 | Test Loss=0.9763 | Test Acc=73.32%\n",
      "[S3-Epoch 038] Train Loss=1.2648 | Test Loss=0.9929 | Test Acc=73.58%\n",
      "[S3-Epoch 039] Train Loss=1.1423 | Test Loss=0.9602 | Test Acc=73.92%\n",
      "[S3-Epoch 040] Train Loss=1.2531 | Test Loss=0.9482 | Test Acc=73.77%\n",
      "[S3-Epoch 041] Train Loss=1.1201 | Test Loss=0.9564 | Test Acc=73.80%\n",
      "[S3-Epoch 042] Train Loss=1.2451 | Test Loss=0.9714 | Test Acc=73.63%\n",
      "[S3-Epoch 043] Train Loss=1.2761 | Test Loss=0.9552 | Test Acc=73.90%\n",
      "[S3-Epoch 044] Train Loss=1.3855 | Test Loss=0.9844 | Test Acc=73.39%\n",
      "[S3-Epoch 045] Train Loss=1.1959 | Test Loss=0.9525 | Test Acc=73.99%\n",
      "[S3-Epoch 046] Train Loss=1.3827 | Test Loss=1.0010 | Test Acc=73.40%\n",
      "[S3-Epoch 047] Train Loss=1.1925 | Test Loss=0.9434 | Test Acc=73.80%\n",
      "[S3-Epoch 048] Train Loss=1.0899 | Test Loss=0.9280 | Test Acc=73.82%\n",
      "[S3-Epoch 049] Train Loss=1.1379 | Test Loss=0.9329 | Test Acc=73.77%\n",
      "[S3-Epoch 050] Train Loss=1.2133 | Test Loss=0.9647 | Test Acc=74.01%\n",
      "[S3-Epoch 051] Train Loss=1.3514 | Test Loss=0.9774 | Test Acc=73.47%\n",
      "[S3-Epoch 052] Train Loss=1.1227 | Test Loss=0.9477 | Test Acc=73.96%\n",
      "[S3-Epoch 053] Train Loss=1.1687 | Test Loss=0.9846 | Test Acc=73.77%\n",
      "[S3-Epoch 054] Train Loss=1.2965 | Test Loss=0.9378 | Test Acc=74.21%\n",
      "[S3-Epoch 055] Train Loss=1.0882 | Test Loss=0.9708 | Test Acc=74.04%\n",
      "[S3-Epoch 056] Train Loss=1.1971 | Test Loss=0.9525 | Test Acc=74.04%\n",
      "[S3-Epoch 057] Train Loss=1.1442 | Test Loss=0.9244 | Test Acc=74.11%\n",
      "[S3-Epoch 058] Train Loss=1.1053 | Test Loss=0.9292 | Test Acc=73.99%\n",
      "[S3-Epoch 059] Train Loss=1.2000 | Test Loss=0.9379 | Test Acc=74.16%\n",
      "[S3-Epoch 060] Train Loss=1.2661 | Test Loss=0.9476 | Test Acc=74.16%\n",
      "[S3-Epoch 061] Train Loss=1.2821 | Test Loss=0.9470 | Test Acc=74.18%\n",
      "[S3-Epoch 062] Train Loss=1.2418 | Test Loss=0.9362 | Test Acc=74.20%\n",
      "[S3-Epoch 063] Train Loss=1.2381 | Test Loss=0.9570 | Test Acc=73.82%\n",
      "[S3-Epoch 064] Train Loss=1.0944 | Test Loss=0.9504 | Test Acc=73.87%\n",
      "[S3-Epoch 065] Train Loss=1.1770 | Test Loss=0.9517 | Test Acc=73.90%\n",
      "[S3-Epoch 066] Train Loss=1.1808 | Test Loss=0.9683 | Test Acc=74.01%\n",
      "[S3-Epoch 067] Train Loss=1.1766 | Test Loss=0.9631 | Test Acc=74.09%\n",
      "[S3-Epoch 068] Train Loss=1.1793 | Test Loss=0.9293 | Test Acc=74.42%\n",
      "[S3-Epoch 069] Train Loss=1.2223 | Test Loss=0.9452 | Test Acc=73.90%\n",
      "[S3-Epoch 070] Train Loss=1.3263 | Test Loss=0.9382 | Test Acc=74.35%\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "results = {}\n",
    "model=VGG16CNN(num_classes=NUM_CLASSES,pretrained=True).to(DEVICE)\n",
    "for name, model_fn in [(\"StandardCNN\", VGG16CNN)]:\n",
    "    model = model_fn().to(DEVICE)\n",
    "    s1, s2, s3 = run_training_3stage(name, model)\n",
    "    df1 = pd.DataFrame(s1, columns=[\"epoch\", \"train_loss\", \"test_loss\", \"test_acc\"])\n",
    "    df1[\"stage\"] = \"Stage 1\"\n",
    "    df1[\"model\"] = name\n",
    "    \n",
    "    df2 = pd.DataFrame(s2, columns=[\"epoch\", \"train_loss\", \"test_loss\", \"test_acc\"])\n",
    "    df2[\"stage\"] = \"Stage 2\"\n",
    "    df2[\"model\"] = name\n",
    "    \n",
    "    df3 = pd.DataFrame(s3, columns=[\"epoch\", \"train_loss\", \"test_loss\", \"test_acc\"])\n",
    "    df3[\"stage\"] = \"Stage 3\"\n",
    "    df3[\"model\"] = name\n",
    "    hist = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b94aec-f2fd-46a9-b327-01ebd03b788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.to_csv(\"BaselineCNN_CUB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e93f60-8da0-4364-9888-88125d9298be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
