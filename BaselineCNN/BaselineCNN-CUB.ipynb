{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18acc869-6481-453a-b7b7-63b78572f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train samples: 256\n",
      "Test samples: 256\n",
      "Model defined (VGG16 backbone).\n",
      "Starting training... Logging to: vgg16_cub_log.csv\n",
      "\n",
      "=== Epoch 1/10 ===\n",
      "[Batch 10/32] loss=3.1529\n",
      "[Batch 20/32] loss=1.8922\n",
      "[Batch 30/32] loss=1.4090\n",
      "Epoch 01/10 | Train Loss: 2.5646 | Val Loss: 2.7350 | Val Acc: 55.47% | Time: 190.4s\n",
      "New best val acc: 55.47%, model saved.\n",
      "\n",
      "=== Epoch 2/10 ===\n",
      "[Batch 10/32] loss=0.8227\n",
      "[Batch 20/32] loss=1.5285\n",
      "[Batch 30/32] loss=0.7787\n",
      "Epoch 02/10 | Train Loss: 0.6187 | Val Loss: 2.7678 | Val Acc: 59.77% | Time: 197.6s\n",
      "New best val acc: 59.77%, model saved.\n",
      "\n",
      "=== Epoch 3/10 ===\n",
      "[Batch 10/32] loss=0.1567\n",
      "[Batch 20/32] loss=0.3537\n",
      "[Batch 30/32] loss=0.3909\n",
      "Epoch 03/10 | Train Loss: 0.3765 | Val Loss: 2.9909 | Val Acc: 64.06% | Time: 201.3s\n",
      "New best val acc: 64.06%, model saved.\n",
      "\n",
      "=== Epoch 4/10 ===\n",
      "[Batch 10/32] loss=0.2683\n",
      "[Batch 20/32] loss=0.2071\n",
      "[Batch 30/32] loss=0.5927\n",
      "Epoch 04/10 | Train Loss: 0.3467 | Val Loss: 2.8854 | Val Acc: 66.80% | Time: 190.8s\n",
      "New best val acc: 66.80%, model saved.\n",
      "\n",
      "=== Epoch 5/10 ===\n",
      "[Batch 10/32] loss=0.2897\n",
      "[Batch 20/32] loss=0.1801\n",
      "[Batch 30/32] loss=0.1809\n",
      "Epoch 05/10 | Train Loss: 0.1984 | Val Loss: 3.1404 | Val Acc: 64.06% | Time: 187.9s\n",
      "\n",
      "=== Epoch 6/10 ===\n",
      "[Batch 10/32] loss=0.0369\n",
      "[Batch 20/32] loss=0.0241\n",
      "[Batch 30/32] loss=0.0515\n",
      "Epoch 06/10 | Train Loss: 0.1476 | Val Loss: 3.4456 | Val Acc: 61.72% | Time: 188.3s\n",
      "\n",
      "=== Epoch 7/10 ===\n",
      "[Batch 10/32] loss=0.0334\n",
      "[Batch 20/32] loss=0.0205\n",
      "[Batch 30/32] loss=0.1932\n",
      "Epoch 07/10 | Train Loss: 0.0993 | Val Loss: 3.6949 | Val Acc: 64.06% | Time: 188.0s\n",
      "\n",
      "=== Epoch 8/10 ===\n",
      "[Batch 10/32] loss=0.0209\n",
      "[Batch 20/32] loss=0.0679\n",
      "[Batch 30/32] loss=0.0029\n",
      "Epoch 08/10 | Train Loss: 0.0960 | Val Loss: 3.9242 | Val Acc: 63.28% | Time: 186.7s\n",
      "\n",
      "=== Epoch 9/10 ===\n",
      "[Batch 10/32] loss=0.0066\n",
      "[Batch 20/32] loss=0.1513\n",
      "[Batch 30/32] loss=0.0717\n",
      "Epoch 09/10 | Train Loss: 0.0698 | Val Loss: 3.6147 | Val Acc: 66.02% | Time: 196.7s\n",
      "\n",
      "=== Epoch 10/10 ===\n",
      "[Batch 10/32] loss=0.2342\n",
      "[Batch 20/32] loss=0.0030\n",
      "[Batch 30/32] loss=0.3807\n",
      "Epoch 10/10 | Train Loss: 0.0723 | Val Loss: 3.9100 | Val Acc: 66.41% | Time: 202.2s\n",
      "\n",
      "Best validation accuracy: 66.80%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# keep PyTorch from using too many CPU threads\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "##########################################\n",
    "##### Dataset class for CUB-200-2011 #####\n",
    "##########################################\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, transform=None, subset_size=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, \"images\")\n",
    "        self.transform = transform\n",
    "\n",
    "        images_path = os.path.join(root_dir, \"images.txt\")\n",
    "        labels_path = os.path.join(root_dir, \"image_class_labels.txt\")\n",
    "        split_path = os.path.join(root_dir, \"train_test_split.txt\")\n",
    "\n",
    "        # id -> relative path\n",
    "        id2img = {}\n",
    "        with open(images_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                img_id, rel_path = line.strip().split()\n",
    "                id2img[int(img_id)] = rel_path\n",
    "\n",
    "        # id -> label (0..199)\n",
    "        id2label = {}\n",
    "        with open(labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                img_id, label = line.strip().split()\n",
    "                id2label[int(img_id)] = int(label) - 1\n",
    "\n",
    "        # id -> train/test flag\n",
    "        id2is_train = {}\n",
    "        with open(split_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                img_id, is_train = line.strip().split()\n",
    "                id2is_train[int(img_id)] = int(is_train)\n",
    "\n",
    "        # build samples list\n",
    "        self.samples = []\n",
    "        for img_id in id2img:\n",
    "            is_train_flag = (id2is_train[img_id] == 1)\n",
    "            if is_train_flag == train:\n",
    "                img_path = os.path.join(self.img_dir, id2img[img_id])\n",
    "                label = id2label[img_id]\n",
    "                self.samples.append((img_path, label))\n",
    "\n",
    "        # subset\n",
    "        if subset_size is not None:\n",
    "            self.samples = self.samples[:subset_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "########################################\n",
    "##### Hyperparameters $ Transforms #####\n",
    "########################################\n",
    "\n",
    "DATA_ROOT = \"data/CUB_200_2011\"   \n",
    "NUM_CLASSES = 200\n",
    "\n",
    "BATCH_SIZE = 8 # keep these small for CPU\n",
    "NUM_WORKERS = 0  # important on macOS\n",
    "\n",
    "DEBUG_TRAIN_SUBSET = 256 # take too long -> opt for subset\n",
    "DEBUG_TEST_SUBSET  = 256   \n",
    "\n",
    "EPOCHS = 10  \n",
    "INIT_LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# ImageNet normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(192),\n",
    "    transforms.RandomResizedCrop(160, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(192),\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "###################################\n",
    "##### Datasets & DataLoaders #####\n",
    "##################################\n",
    "\n",
    "train_dataset = CUBDataset(\n",
    "    DATA_ROOT,\n",
    "    train=True,\n",
    "    transform=train_transform,\n",
    "    subset_size=DEBUG_TRAIN_SUBSET,\n",
    ")\n",
    "\n",
    "test_dataset = CUBDataset(\n",
    "    DATA_ROOT,\n",
    "    train=False,\n",
    "    transform=test_transform,\n",
    "    subset_size=DEBUG_TEST_SUBSET,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))\n",
    "\n",
    "################################\n",
    "##### VGG16 Backbone Model #####\n",
    "################################\n",
    "class VGG16CUB(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, pretrained=True, freeze_features=True):\n",
    "        super().__init__()\n",
    "        # Load pre-trained VGG16\n",
    "        vgg = models.vgg16(\n",
    "            weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        )\n",
    "\n",
    "        # Freeze convolutional layers (faster on CPU)\n",
    "        if freeze_features:\n",
    "            for param in vgg.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Replace final classifier layer \n",
    "        in_features = vgg.classifier[6].in_features \n",
    "        vgg.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        self.model = vgg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = VGG16CUB(num_classes=NUM_CLASSES, pretrained=True, freeze_features=True).to(DEVICE)\n",
    "print(\"Model defined (VGG16 backbone).\")\n",
    "\n",
    "###########################################\n",
    "##### Training / Evaluation Functions #####\n",
    "###########################################\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Progress print\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"[Batch {batch_idx+1}/{len(loader)}] loss={loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(loader.dataset)\n",
    "    return accuracy, epoch_loss\n",
    "\n",
    "########################################\n",
    "##### Loss, Optimizer, CSV Logging #####\n",
    "########################################\n",
    "\n",
    "SEED = 542\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only optimize parameters that require grad\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=INIT_LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "best_acc = 0.0\n",
    "log_path = \"vgg16_cub_log.csv\"\n",
    "\n",
    "# Prepare CSV log for plotting later\n",
    "with open(log_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\"])\n",
    "\n",
    "print(\"Starting training... Logging to:\", log_path)\n",
    "\n",
    "#########################\n",
    "##### Training Loop #####\n",
    "#########################\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_acc, val_loss = evaluate(model, test_loader, criterion, DEVICE)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.2f}% | \"\n",
    "        f\"Time: {elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    # Append to CSV\n",
    "    with open(log_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, train_loss, val_loss, val_acc])\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"vgg16_cub_best.pth\")\n",
    "        print(f\"New best val acc: {best_acc:.2f}%, model saved.\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292054d5-eff3-40ef-a0b2-f853fccae7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
