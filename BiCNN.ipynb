{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc6a5221-7a25-4913-b463-8fc1be878742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38afb1ab-a1f5-41a8-aa51-bb4fdfb4c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_ROOT = \"data/CUB_200_2011\"\n",
    "NUM_CLASSES = 200\n",
    "INIT_LR = 1e-4\n",
    "MID_LR = 1e-5\n",
    "FINAL_LR = 1e-6\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "STEP_SIZE = 7\n",
    "EPOCHS_STAGE1 = 10\n",
    "EPOCHS_STAGE2 = 100\n",
    "EPOCHS_STAGE3 = 200\n",
    "SEED = 87\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aecfc580-fdd3-4717-84f9-ea312fa52053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        img_txt = os.path.join(root, \"images.txt\")\n",
    "        label_txt = os.path.join(root, \"image_class_labels.txt\")\n",
    "        split_txt = os.path.join(root, \"train_test_split.txt\")\n",
    "\n",
    "        with open(img_txt) as f:\n",
    "            imgs = [x.strip().split(\" \") for x in f.readlines()]\n",
    "        with open(label_txt) as f:\n",
    "            labels = [int(x.strip().split(\" \")[1]) - 1 for x in f.readlines()]\n",
    "        with open(split_txt) as f:\n",
    "            split = [int(x.strip().split(\" \")[1]) for x in f.readlines()]\n",
    "\n",
    "        self.samples = []\n",
    "        for (img_id, img_path), label, is_train in zip(imgs, labels, split):\n",
    "            if (train and is_train == 1) or (not train and is_train == 0):\n",
    "                self.samples.append((os.path.join(root, \"images\", img_path), label))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    CUBDataset(DATA_ROOT, True, train_transforms),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "    pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    CUBDataset(DATA_ROOT, False, test_transforms),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n",
    "    pin_memory=True, persistent_workers=True, prefetch_factor=4\n",
    ")\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return 100 * correct / total, total_loss / total\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, total = 0.0, 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        # MixUp\n",
    "        mixed_imgs, y_a, y_b, lam = mixup_data(images, labels, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_imgs)\n",
    "        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "\n",
    "        # correctly accumulate *sample-level* loss\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return running_loss / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48549210-42e6-454e-b3e5-be175d1b748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        backbone = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "        self.features = nn.Sequential(*list(backbone.features.children())[:-1])\n",
    "        self.num_features = 512\n",
    "        self.fc = nn.Linear(self.num_features * self.num_features, num_classes)\n",
    "        nn.init.kaiming_uniform_(self.fc.weight)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.view(B, C, H * W)\n",
    "        bilinear = torch.bmm(x, x.transpose(1, 2)) / (H * W)\n",
    "        bilinear = bilinear.view(B, C * C)\n",
    "        bilinear = torch.sign(bilinear) * torch.sqrt(torch.abs(bilinear) + 1e-5)\n",
    "        bilinear = nn.functional.normalize(bilinear)\n",
    "        x = self.dropout(bilinear)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "968721e4-bcbc-4b9b-b371-b8723ac449ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_3stage(model_name, model):\n",
    "    print(\"=== Training ===\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ============================================================\n",
    "    # Stage 1: Train classifier only (linear probing)\n",
    "    # ============================================================\n",
    "    for p in model.parameters(): \n",
    "        p.requires_grad = False\n",
    "    for p in model.fc.parameters(): \n",
    "        p.requires_grad = True\n",
    "\n",
    "    print(\"--- Stage 1 ---\")\n",
    "    optimizer = optim.AdamW(model.fc.parameters(), lr=INIT_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=0.1)\n",
    "\n",
    "    stage1_hist = []\n",
    "    for epoch in range(EPOCHS_STAGE1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "\n",
    "        stage1_hist.append((epoch+1, tr_loss, te_loss, te_acc))\n",
    "\n",
    "        print(f\"[S1-Epoch {epoch+1:02d}] \"\n",
    "              f\"Train Loss={tr_loss:.4f} | Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Stage 2: Fine-tune entire network (moderate LR)\n",
    "    # ============================================================\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    print(\"--- Stage 2---\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=MID_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=EPOCHS_STAGE2, gamma=0.1)\n",
    "\n",
    "    stage2_hist = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS_STAGE2):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "\n",
    "        stage2_hist.append((epoch+1, tr_loss, te_loss, te_acc))\n",
    "\n",
    "        if te_acc > best_acc:\n",
    "            best_acc = te_acc\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pt\")\n",
    "\n",
    "        print(f\"[S2-Epoch {epoch+1:03d}] \"\n",
    "              f\"Train Loss={tr_loss:.4f} | Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Stage 3: Low-LR refinement (very small LR)\n",
    "    # ============================================================\n",
    "    print(\"--- Stage 3---\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=FINAL_LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=EPOCHS_STAGE3, gamma=0.1)\n",
    "\n",
    "    stage3_hist = []\n",
    "\n",
    "    for epoch in range(EPOCHS_STAGE3):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "        scheduler.step()\n",
    "\n",
    "        stage3_hist.append((epoch+1, tr_loss, te_loss, te_acc))\n",
    "\n",
    "        print(f\"[S3-Epoch {epoch+1:03d}] \"\n",
    "              f\"Train Loss={tr_loss:.4f} | Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    return stage1_hist, stage2_hist, stage3_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39115bd8-d46d-4e4e-ac87-2f8030a04b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n",
      "--- Stage 1 ---\n",
      "[S1-Epoch 01] Train Loss=5.2403 | Test Loss=5.0745 | Test Acc=26.89%\n",
      "[S1-Epoch 02] Train Loss=5.0617 | Test Loss=4.8950 | Test Acc=41.78%\n",
      "[S1-Epoch 03] Train Loss=4.8951 | Test Loss=4.7143 | Test Acc=47.62%\n",
      "[S1-Epoch 04] Train Loss=4.7619 | Test Loss=4.5514 | Test Acc=51.54%\n",
      "[S1-Epoch 05] Train Loss=4.6401 | Test Loss=4.4004 | Test Acc=52.23%\n",
      "[S1-Epoch 06] Train Loss=4.4997 | Test Loss=4.2515 | Test Acc=54.68%\n",
      "[S1-Epoch 07] Train Loss=4.4218 | Test Loss=4.1212 | Test Acc=54.92%\n",
      "[S1-Epoch 08] Train Loss=4.3078 | Test Loss=4.1163 | Test Acc=54.71%\n",
      "[S1-Epoch 09] Train Loss=4.2940 | Test Loss=4.1018 | Test Acc=55.63%\n",
      "[S1-Epoch 10] Train Loss=4.2788 | Test Loss=4.0862 | Test Acc=56.39%\n",
      "--- Stage 2---\n",
      "[S2-Epoch 001] Train Loss=3.9967 | Test Loss=3.6166 | Test Acc=55.89%\n",
      "[S2-Epoch 002] Train Loss=3.7812 | Test Loss=3.4784 | Test Acc=56.25%\n",
      "[S2-Epoch 003] Train Loss=3.6744 | Test Loss=3.3825 | Test Acc=58.34%\n",
      "[S2-Epoch 004] Train Loss=3.5669 | Test Loss=3.3010 | Test Acc=59.92%\n",
      "[S2-Epoch 005] Train Loss=3.4936 | Test Loss=3.2260 | Test Acc=61.56%\n",
      "[S2-Epoch 006] Train Loss=3.5173 | Test Loss=3.1620 | Test Acc=62.31%\n",
      "[S2-Epoch 007] Train Loss=3.4575 | Test Loss=3.1013 | Test Acc=63.39%\n",
      "[S2-Epoch 008] Train Loss=3.3917 | Test Loss=3.0445 | Test Acc=64.12%\n",
      "[S2-Epoch 009] Train Loss=3.3220 | Test Loss=2.9833 | Test Acc=65.34%\n",
      "[S2-Epoch 010] Train Loss=3.2576 | Test Loss=2.9263 | Test Acc=66.26%\n",
      "[S2-Epoch 011] Train Loss=3.2083 | Test Loss=2.8754 | Test Acc=65.88%\n",
      "[S2-Epoch 012] Train Loss=3.1630 | Test Loss=2.8156 | Test Acc=67.29%\n",
      "[S2-Epoch 013] Train Loss=3.0928 | Test Loss=2.7582 | Test Acc=67.60%\n",
      "[S2-Epoch 014] Train Loss=3.0341 | Test Loss=2.7082 | Test Acc=68.43%\n",
      "[S2-Epoch 015] Train Loss=3.0078 | Test Loss=2.6596 | Test Acc=69.24%\n",
      "[S2-Epoch 016] Train Loss=2.9400 | Test Loss=2.6076 | Test Acc=69.23%\n",
      "[S2-Epoch 017] Train Loss=2.9257 | Test Loss=2.5571 | Test Acc=69.74%\n",
      "[S2-Epoch 018] Train Loss=2.8413 | Test Loss=2.5087 | Test Acc=70.31%\n",
      "[S2-Epoch 019] Train Loss=2.8508 | Test Loss=2.4659 | Test Acc=70.68%\n",
      "[S2-Epoch 020] Train Loss=2.7181 | Test Loss=2.4138 | Test Acc=71.06%\n",
      "[S2-Epoch 021] Train Loss=2.7008 | Test Loss=2.3676 | Test Acc=71.42%\n",
      "[S2-Epoch 022] Train Loss=2.6524 | Test Loss=2.3155 | Test Acc=72.25%\n",
      "[S2-Epoch 023] Train Loss=2.6471 | Test Loss=2.2696 | Test Acc=72.97%\n",
      "[S2-Epoch 024] Train Loss=2.6349 | Test Loss=2.2273 | Test Acc=73.18%\n",
      "[S2-Epoch 025] Train Loss=2.6444 | Test Loss=2.1898 | Test Acc=73.65%\n",
      "[S2-Epoch 026] Train Loss=2.4123 | Test Loss=2.1442 | Test Acc=73.82%\n",
      "[S2-Epoch 027] Train Loss=2.4821 | Test Loss=2.1014 | Test Acc=74.16%\n",
      "[S2-Epoch 028] Train Loss=2.4129 | Test Loss=2.0590 | Test Acc=74.51%\n",
      "[S2-Epoch 029] Train Loss=2.3748 | Test Loss=2.0219 | Test Acc=74.82%\n",
      "[S2-Epoch 030] Train Loss=2.3200 | Test Loss=1.9858 | Test Acc=75.27%\n",
      "[S2-Epoch 031] Train Loss=2.2955 | Test Loss=1.9445 | Test Acc=75.58%\n",
      "[S2-Epoch 032] Train Loss=2.2342 | Test Loss=1.9058 | Test Acc=75.94%\n",
      "[S2-Epoch 033] Train Loss=2.2401 | Test Loss=1.8671 | Test Acc=76.56%\n",
      "[S2-Epoch 034] Train Loss=2.1802 | Test Loss=1.8290 | Test Acc=76.39%\n",
      "[S2-Epoch 035] Train Loss=2.0870 | Test Loss=1.7934 | Test Acc=76.72%\n",
      "[S2-Epoch 036] Train Loss=2.0974 | Test Loss=1.7676 | Test Acc=77.24%\n",
      "[S2-Epoch 037] Train Loss=2.0539 | Test Loss=1.7326 | Test Acc=77.08%\n",
      "[S2-Epoch 038] Train Loss=1.9981 | Test Loss=1.6941 | Test Acc=77.65%\n",
      "[S2-Epoch 039] Train Loss=2.0002 | Test Loss=1.6670 | Test Acc=77.36%\n",
      "[S2-Epoch 040] Train Loss=1.9831 | Test Loss=1.6368 | Test Acc=77.93%\n",
      "[S2-Epoch 041] Train Loss=2.0096 | Test Loss=1.6096 | Test Acc=78.12%\n",
      "[S2-Epoch 042] Train Loss=1.9967 | Test Loss=1.5823 | Test Acc=78.17%\n",
      "[S2-Epoch 043] Train Loss=1.9021 | Test Loss=1.5477 | Test Acc=78.29%\n",
      "[S2-Epoch 044] Train Loss=1.8958 | Test Loss=1.5236 | Test Acc=78.63%\n",
      "[S2-Epoch 045] Train Loss=1.8369 | Test Loss=1.4959 | Test Acc=78.75%\n",
      "[S2-Epoch 046] Train Loss=1.8564 | Test Loss=1.4768 | Test Acc=78.49%\n",
      "[S2-Epoch 047] Train Loss=1.7704 | Test Loss=1.4508 | Test Acc=79.00%\n",
      "[S2-Epoch 048] Train Loss=1.6666 | Test Loss=1.4233 | Test Acc=79.20%\n",
      "[S2-Epoch 049] Train Loss=1.6731 | Test Loss=1.3988 | Test Acc=79.27%\n",
      "[S2-Epoch 050] Train Loss=1.7374 | Test Loss=1.3829 | Test Acc=79.34%\n",
      "[S2-Epoch 051] Train Loss=1.6911 | Test Loss=1.3557 | Test Acc=79.70%\n",
      "[S2-Epoch 052] Train Loss=1.7316 | Test Loss=1.3382 | Test Acc=79.69%\n",
      "[S2-Epoch 053] Train Loss=1.6548 | Test Loss=1.3151 | Test Acc=80.00%\n",
      "[S2-Epoch 054] Train Loss=1.7011 | Test Loss=1.3007 | Test Acc=79.69%\n",
      "[S2-Epoch 055] Train Loss=1.7072 | Test Loss=1.2837 | Test Acc=79.84%\n",
      "[S2-Epoch 056] Train Loss=1.6710 | Test Loss=1.2704 | Test Acc=80.13%\n",
      "[S2-Epoch 057] Train Loss=1.5569 | Test Loss=1.2421 | Test Acc=80.20%\n",
      "[S2-Epoch 058] Train Loss=1.4941 | Test Loss=1.2212 | Test Acc=80.55%\n",
      "[S2-Epoch 059] Train Loss=1.3908 | Test Loss=1.2058 | Test Acc=80.10%\n",
      "[S2-Epoch 060] Train Loss=1.3355 | Test Loss=1.1844 | Test Acc=80.53%\n",
      "[S2-Epoch 061] Train Loss=1.4597 | Test Loss=1.1732 | Test Acc=80.53%\n",
      "[S2-Epoch 062] Train Loss=1.4929 | Test Loss=1.1629 | Test Acc=80.84%\n",
      "[S2-Epoch 063] Train Loss=1.4165 | Test Loss=1.1424 | Test Acc=80.76%\n",
      "[S2-Epoch 064] Train Loss=1.3392 | Test Loss=1.1295 | Test Acc=80.65%\n",
      "[S2-Epoch 065] Train Loss=1.2990 | Test Loss=1.1139 | Test Acc=80.77%\n",
      "[S2-Epoch 066] Train Loss=1.4508 | Test Loss=1.1057 | Test Acc=81.05%\n",
      "[S2-Epoch 067] Train Loss=1.2416 | Test Loss=1.0826 | Test Acc=81.07%\n",
      "[S2-Epoch 068] Train Loss=1.3015 | Test Loss=1.0777 | Test Acc=81.46%\n",
      "[S2-Epoch 069] Train Loss=1.2355 | Test Loss=1.0570 | Test Acc=81.39%\n",
      "[S2-Epoch 070] Train Loss=1.3562 | Test Loss=1.0534 | Test Acc=81.27%\n",
      "[S2-Epoch 071] Train Loss=1.2498 | Test Loss=1.0414 | Test Acc=81.55%\n",
      "[S2-Epoch 072] Train Loss=1.3437 | Test Loss=1.0311 | Test Acc=81.58%\n",
      "[S2-Epoch 073] Train Loss=1.2093 | Test Loss=1.0214 | Test Acc=81.33%\n",
      "[S2-Epoch 074] Train Loss=1.3076 | Test Loss=1.0096 | Test Acc=81.33%\n",
      "[S2-Epoch 075] Train Loss=1.2469 | Test Loss=1.0106 | Test Acc=81.39%\n",
      "[S2-Epoch 076] Train Loss=1.1881 | Test Loss=0.9888 | Test Acc=81.64%\n",
      "[S2-Epoch 077] Train Loss=1.2261 | Test Loss=0.9897 | Test Acc=81.38%\n",
      "[S2-Epoch 078] Train Loss=1.3620 | Test Loss=0.9785 | Test Acc=81.74%\n",
      "[S2-Epoch 079] Train Loss=1.2048 | Test Loss=0.9770 | Test Acc=81.65%\n",
      "[S2-Epoch 080] Train Loss=1.2711 | Test Loss=0.9584 | Test Acc=81.74%\n",
      "[S2-Epoch 081] Train Loss=1.1789 | Test Loss=0.9534 | Test Acc=81.84%\n",
      "[S2-Epoch 082] Train Loss=1.2549 | Test Loss=0.9483 | Test Acc=81.69%\n",
      "[S2-Epoch 083] Train Loss=1.1135 | Test Loss=0.9364 | Test Acc=82.12%\n",
      "[S2-Epoch 084] Train Loss=1.2216 | Test Loss=0.9409 | Test Acc=81.71%\n",
      "[S2-Epoch 085] Train Loss=1.1453 | Test Loss=0.9252 | Test Acc=81.96%\n",
      "[S2-Epoch 086] Train Loss=1.2152 | Test Loss=0.9249 | Test Acc=81.65%\n",
      "[S2-Epoch 087] Train Loss=1.0684 | Test Loss=0.9092 | Test Acc=81.93%\n",
      "[S2-Epoch 088] Train Loss=1.0987 | Test Loss=0.9083 | Test Acc=81.74%\n",
      "[S2-Epoch 089] Train Loss=1.1352 | Test Loss=0.9032 | Test Acc=81.48%\n",
      "[S2-Epoch 090] Train Loss=1.1037 | Test Loss=0.8968 | Test Acc=81.60%\n",
      "[S2-Epoch 091] Train Loss=1.1306 | Test Loss=0.8917 | Test Acc=81.19%\n",
      "[S2-Epoch 092] Train Loss=1.0824 | Test Loss=0.8868 | Test Acc=81.62%\n",
      "[S2-Epoch 093] Train Loss=1.0551 | Test Loss=0.8817 | Test Acc=81.50%\n",
      "[S2-Epoch 094] Train Loss=1.0350 | Test Loss=0.8801 | Test Acc=81.67%\n",
      "[S2-Epoch 095] Train Loss=1.0204 | Test Loss=0.8712 | Test Acc=81.84%\n",
      "[S2-Epoch 096] Train Loss=1.0537 | Test Loss=0.8659 | Test Acc=81.95%\n",
      "[S2-Epoch 097] Train Loss=1.0465 | Test Loss=0.8622 | Test Acc=81.81%\n",
      "[S2-Epoch 098] Train Loss=1.0453 | Test Loss=0.8631 | Test Acc=81.62%\n",
      "[S2-Epoch 099] Train Loss=1.0701 | Test Loss=0.8592 | Test Acc=81.53%\n",
      "[S2-Epoch 100] Train Loss=1.0589 | Test Loss=0.8495 | Test Acc=81.69%\n",
      "--- Stage 3---\n",
      "[S3-Epoch 001] Train Loss=1.1749 | Test Loss=0.8558 | Test Acc=81.69%\n",
      "[S3-Epoch 002] Train Loss=1.0736 | Test Loss=0.8477 | Test Acc=81.84%\n",
      "[S3-Epoch 003] Train Loss=0.9406 | Test Loss=0.8486 | Test Acc=81.91%\n",
      "[S3-Epoch 004] Train Loss=0.9515 | Test Loss=0.8523 | Test Acc=81.86%\n",
      "[S3-Epoch 005] Train Loss=0.9914 | Test Loss=0.8501 | Test Acc=81.86%\n",
      "[S3-Epoch 006] Train Loss=0.9516 | Test Loss=0.8426 | Test Acc=81.64%\n",
      "[S3-Epoch 007] Train Loss=1.0915 | Test Loss=0.8465 | Test Acc=81.96%\n",
      "[S3-Epoch 008] Train Loss=1.1158 | Test Loss=0.8480 | Test Acc=81.74%\n",
      "[S3-Epoch 009] Train Loss=0.9720 | Test Loss=0.8455 | Test Acc=81.96%\n",
      "[S3-Epoch 010] Train Loss=0.9845 | Test Loss=0.8464 | Test Acc=82.02%\n",
      "[S3-Epoch 011] Train Loss=0.9319 | Test Loss=0.8417 | Test Acc=81.84%\n",
      "[S3-Epoch 012] Train Loss=0.9714 | Test Loss=0.8396 | Test Acc=81.84%\n",
      "[S3-Epoch 013] Train Loss=0.9613 | Test Loss=0.8382 | Test Acc=81.96%\n",
      "[S3-Epoch 014] Train Loss=1.0263 | Test Loss=0.8398 | Test Acc=81.81%\n",
      "[S3-Epoch 015] Train Loss=0.9880 | Test Loss=0.8412 | Test Acc=81.95%\n",
      "[S3-Epoch 016] Train Loss=1.0837 | Test Loss=0.8409 | Test Acc=81.86%\n",
      "[S3-Epoch 017] Train Loss=0.8554 | Test Loss=0.8385 | Test Acc=81.95%\n",
      "[S3-Epoch 018] Train Loss=1.0612 | Test Loss=0.8409 | Test Acc=81.95%\n",
      "[S3-Epoch 019] Train Loss=1.0134 | Test Loss=0.8451 | Test Acc=81.84%\n",
      "[S3-Epoch 020] Train Loss=0.9938 | Test Loss=0.8403 | Test Acc=81.95%\n",
      "[S3-Epoch 021] Train Loss=1.0050 | Test Loss=0.8395 | Test Acc=81.93%\n",
      "[S3-Epoch 022] Train Loss=1.0689 | Test Loss=0.8344 | Test Acc=82.08%\n",
      "[S3-Epoch 023] Train Loss=1.0551 | Test Loss=0.8352 | Test Acc=81.95%\n",
      "[S3-Epoch 024] Train Loss=1.0542 | Test Loss=0.8392 | Test Acc=82.07%\n",
      "[S3-Epoch 025] Train Loss=1.0889 | Test Loss=0.8440 | Test Acc=82.00%\n",
      "[S3-Epoch 026] Train Loss=1.0093 | Test Loss=0.8394 | Test Acc=81.95%\n",
      "[S3-Epoch 027] Train Loss=0.9938 | Test Loss=0.8384 | Test Acc=82.08%\n",
      "[S3-Epoch 028] Train Loss=0.9766 | Test Loss=0.8315 | Test Acc=81.98%\n",
      "[S3-Epoch 029] Train Loss=0.8515 | Test Loss=0.8323 | Test Acc=82.07%\n",
      "[S3-Epoch 030] Train Loss=0.9930 | Test Loss=0.8347 | Test Acc=81.90%\n",
      "[S3-Epoch 031] Train Loss=0.9824 | Test Loss=0.8458 | Test Acc=81.93%\n",
      "[S3-Epoch 032] Train Loss=1.0418 | Test Loss=0.8338 | Test Acc=82.02%\n",
      "[S3-Epoch 033] Train Loss=1.0322 | Test Loss=0.8349 | Test Acc=81.93%\n",
      "[S3-Epoch 034] Train Loss=1.0321 | Test Loss=0.8373 | Test Acc=81.91%\n",
      "[S3-Epoch 035] Train Loss=0.9984 | Test Loss=0.8438 | Test Acc=81.83%\n",
      "[S3-Epoch 036] Train Loss=0.9336 | Test Loss=0.8286 | Test Acc=82.12%\n",
      "[S3-Epoch 037] Train Loss=1.0769 | Test Loss=0.8312 | Test Acc=81.93%\n",
      "[S3-Epoch 038] Train Loss=0.9982 | Test Loss=0.8400 | Test Acc=81.91%\n",
      "[S3-Epoch 039] Train Loss=0.8228 | Test Loss=0.8249 | Test Acc=82.03%\n",
      "[S3-Epoch 040] Train Loss=0.9236 | Test Loss=0.8295 | Test Acc=82.00%\n",
      "[S3-Epoch 041] Train Loss=1.0285 | Test Loss=0.8271 | Test Acc=82.17%\n",
      "[S3-Epoch 042] Train Loss=0.9650 | Test Loss=0.8295 | Test Acc=82.14%\n",
      "[S3-Epoch 043] Train Loss=0.9345 | Test Loss=0.8267 | Test Acc=82.12%\n",
      "[S3-Epoch 044] Train Loss=1.0825 | Test Loss=0.8287 | Test Acc=82.14%\n",
      "[S3-Epoch 045] Train Loss=0.9372 | Test Loss=0.8314 | Test Acc=81.98%\n",
      "[S3-Epoch 046] Train Loss=0.9786 | Test Loss=0.8301 | Test Acc=82.03%\n",
      "[S3-Epoch 047] Train Loss=0.9811 | Test Loss=0.8338 | Test Acc=81.95%\n",
      "[S3-Epoch 048] Train Loss=0.9890 | Test Loss=0.8235 | Test Acc=81.95%\n",
      "[S3-Epoch 049] Train Loss=1.0800 | Test Loss=0.8252 | Test Acc=82.08%\n",
      "[S3-Epoch 050] Train Loss=1.0871 | Test Loss=0.8228 | Test Acc=82.14%\n",
      "[S3-Epoch 051] Train Loss=0.8555 | Test Loss=0.8263 | Test Acc=82.15%\n",
      "[S3-Epoch 052] Train Loss=0.9407 | Test Loss=0.8258 | Test Acc=81.88%\n",
      "[S3-Epoch 053] Train Loss=0.9720 | Test Loss=0.8229 | Test Acc=82.00%\n",
      "[S3-Epoch 054] Train Loss=1.0201 | Test Loss=0.8227 | Test Acc=81.77%\n",
      "[S3-Epoch 055] Train Loss=1.0582 | Test Loss=0.8318 | Test Acc=81.64%\n",
      "[S3-Epoch 056] Train Loss=0.9293 | Test Loss=0.8192 | Test Acc=82.03%\n",
      "[S3-Epoch 057] Train Loss=0.9190 | Test Loss=0.8216 | Test Acc=82.03%\n",
      "[S3-Epoch 058] Train Loss=0.9873 | Test Loss=0.8299 | Test Acc=81.95%\n",
      "[S3-Epoch 059] Train Loss=1.0064 | Test Loss=0.8291 | Test Acc=81.95%\n",
      "[S3-Epoch 060] Train Loss=0.8862 | Test Loss=0.8251 | Test Acc=81.91%\n",
      "[S3-Epoch 061] Train Loss=0.9102 | Test Loss=0.8249 | Test Acc=82.02%\n",
      "[S3-Epoch 062] Train Loss=0.9845 | Test Loss=0.8247 | Test Acc=82.12%\n",
      "[S3-Epoch 063] Train Loss=0.9269 | Test Loss=0.8208 | Test Acc=82.03%\n",
      "[S3-Epoch 064] Train Loss=1.0354 | Test Loss=0.8228 | Test Acc=82.07%\n",
      "[S3-Epoch 065] Train Loss=0.9447 | Test Loss=0.8162 | Test Acc=82.03%\n",
      "[S3-Epoch 066] Train Loss=0.9938 | Test Loss=0.8265 | Test Acc=81.91%\n",
      "[S3-Epoch 067] Train Loss=0.9551 | Test Loss=0.8228 | Test Acc=82.05%\n",
      "[S3-Epoch 068] Train Loss=0.9499 | Test Loss=0.8274 | Test Acc=81.95%\n",
      "[S3-Epoch 069] Train Loss=0.9770 | Test Loss=0.8224 | Test Acc=82.05%\n",
      "[S3-Epoch 070] Train Loss=0.9476 | Test Loss=0.8157 | Test Acc=81.98%\n",
      "[S3-Epoch 071] Train Loss=0.8699 | Test Loss=0.8171 | Test Acc=82.12%\n",
      "[S3-Epoch 072] Train Loss=1.0882 | Test Loss=0.8181 | Test Acc=82.07%\n",
      "[S3-Epoch 073] Train Loss=1.1042 | Test Loss=0.8208 | Test Acc=82.10%\n",
      "[S3-Epoch 074] Train Loss=0.8925 | Test Loss=0.8216 | Test Acc=81.98%\n",
      "[S3-Epoch 075] Train Loss=1.1001 | Test Loss=0.8254 | Test Acc=82.02%\n",
      "[S3-Epoch 076] Train Loss=0.9755 | Test Loss=0.8247 | Test Acc=82.05%\n",
      "[S3-Epoch 077] Train Loss=1.0201 | Test Loss=0.8207 | Test Acc=81.88%\n",
      "[S3-Epoch 078] Train Loss=0.9893 | Test Loss=0.8225 | Test Acc=81.86%\n",
      "[S3-Epoch 079] Train Loss=1.0490 | Test Loss=0.8209 | Test Acc=81.96%\n",
      "[S3-Epoch 080] Train Loss=1.0833 | Test Loss=0.8173 | Test Acc=81.90%\n",
      "[S3-Epoch 081] Train Loss=0.9929 | Test Loss=0.8153 | Test Acc=82.12%\n",
      "[S3-Epoch 082] Train Loss=0.9861 | Test Loss=0.8167 | Test Acc=82.08%\n",
      "[S3-Epoch 083] Train Loss=1.0093 | Test Loss=0.8187 | Test Acc=81.98%\n",
      "[S3-Epoch 084] Train Loss=0.9492 | Test Loss=0.8153 | Test Acc=82.10%\n",
      "[S3-Epoch 085] Train Loss=0.9779 | Test Loss=0.8197 | Test Acc=82.12%\n",
      "[S3-Epoch 086] Train Loss=0.9779 | Test Loss=0.8159 | Test Acc=81.98%\n",
      "[S3-Epoch 087] Train Loss=1.0174 | Test Loss=0.8212 | Test Acc=82.12%\n",
      "[S3-Epoch 088] Train Loss=1.0365 | Test Loss=0.8156 | Test Acc=82.03%\n",
      "[S3-Epoch 089] Train Loss=1.0017 | Test Loss=0.8145 | Test Acc=81.93%\n",
      "[S3-Epoch 090] Train Loss=0.8536 | Test Loss=0.8165 | Test Acc=81.83%\n",
      "[S3-Epoch 091] Train Loss=1.0435 | Test Loss=0.8131 | Test Acc=81.95%\n",
      "[S3-Epoch 092] Train Loss=0.9401 | Test Loss=0.8158 | Test Acc=82.05%\n",
      "[S3-Epoch 093] Train Loss=0.8053 | Test Loss=0.8137 | Test Acc=81.98%\n",
      "[S3-Epoch 094] Train Loss=0.9646 | Test Loss=0.8092 | Test Acc=82.08%\n",
      "[S3-Epoch 095] Train Loss=0.9884 | Test Loss=0.8095 | Test Acc=82.22%\n",
      "[S3-Epoch 096] Train Loss=0.9489 | Test Loss=0.8125 | Test Acc=81.86%\n",
      "[S3-Epoch 097] Train Loss=0.9479 | Test Loss=0.8286 | Test Acc=81.90%\n",
      "[S3-Epoch 098] Train Loss=0.9715 | Test Loss=0.8127 | Test Acc=82.15%\n",
      "[S3-Epoch 099] Train Loss=0.9903 | Test Loss=0.8168 | Test Acc=82.15%\n",
      "[S3-Epoch 100] Train Loss=0.9454 | Test Loss=0.8094 | Test Acc=82.17%\n",
      "[S3-Epoch 101] Train Loss=1.0062 | Test Loss=0.8194 | Test Acc=82.08%\n",
      "[S3-Epoch 102] Train Loss=0.9730 | Test Loss=0.8190 | Test Acc=81.98%\n",
      "[S3-Epoch 103] Train Loss=1.0259 | Test Loss=0.8218 | Test Acc=81.91%\n",
      "[S3-Epoch 104] Train Loss=0.9031 | Test Loss=0.8110 | Test Acc=82.12%\n",
      "[S3-Epoch 105] Train Loss=0.9300 | Test Loss=0.8107 | Test Acc=82.02%\n",
      "[S3-Epoch 106] Train Loss=1.0014 | Test Loss=0.8086 | Test Acc=82.07%\n",
      "[S3-Epoch 107] Train Loss=1.0107 | Test Loss=0.8107 | Test Acc=82.05%\n",
      "[S3-Epoch 108] Train Loss=1.0210 | Test Loss=0.8169 | Test Acc=82.00%\n",
      "[S3-Epoch 109] Train Loss=0.9938 | Test Loss=0.8150 | Test Acc=82.00%\n",
      "[S3-Epoch 110] Train Loss=0.8042 | Test Loss=0.8079 | Test Acc=82.00%\n",
      "[S3-Epoch 111] Train Loss=1.0638 | Test Loss=0.8150 | Test Acc=81.98%\n",
      "[S3-Epoch 112] Train Loss=1.0734 | Test Loss=0.8148 | Test Acc=82.03%\n",
      "[S3-Epoch 113] Train Loss=0.9265 | Test Loss=0.8109 | Test Acc=81.79%\n",
      "[S3-Epoch 114] Train Loss=0.9605 | Test Loss=0.8188 | Test Acc=81.90%\n",
      "[S3-Epoch 115] Train Loss=0.9915 | Test Loss=0.8159 | Test Acc=82.05%\n",
      "[S3-Epoch 116] Train Loss=1.0370 | Test Loss=0.8151 | Test Acc=81.95%\n",
      "[S3-Epoch 117] Train Loss=0.9876 | Test Loss=0.8062 | Test Acc=82.08%\n",
      "[S3-Epoch 118] Train Loss=0.9876 | Test Loss=0.8137 | Test Acc=82.02%\n",
      "[S3-Epoch 119] Train Loss=1.0252 | Test Loss=0.8116 | Test Acc=82.07%\n",
      "[S3-Epoch 120] Train Loss=1.0458 | Test Loss=0.8059 | Test Acc=82.00%\n",
      "[S3-Epoch 121] Train Loss=0.9638 | Test Loss=0.8097 | Test Acc=82.03%\n",
      "[S3-Epoch 122] Train Loss=0.9893 | Test Loss=0.8172 | Test Acc=81.98%\n",
      "[S3-Epoch 123] Train Loss=0.9499 | Test Loss=0.8103 | Test Acc=82.14%\n",
      "[S3-Epoch 124] Train Loss=1.0352 | Test Loss=0.8075 | Test Acc=82.07%\n",
      "[S3-Epoch 125] Train Loss=0.8728 | Test Loss=0.8047 | Test Acc=81.86%\n",
      "[S3-Epoch 126] Train Loss=0.8393 | Test Loss=0.8024 | Test Acc=82.12%\n",
      "[S3-Epoch 127] Train Loss=0.9684 | Test Loss=0.8035 | Test Acc=82.12%\n",
      "[S3-Epoch 128] Train Loss=0.8876 | Test Loss=0.8048 | Test Acc=82.02%\n",
      "[S3-Epoch 129] Train Loss=0.9482 | Test Loss=0.8023 | Test Acc=82.12%\n",
      "[S3-Epoch 130] Train Loss=0.9700 | Test Loss=0.8090 | Test Acc=81.90%\n",
      "[S3-Epoch 131] Train Loss=0.8442 | Test Loss=0.8041 | Test Acc=82.05%\n",
      "[S3-Epoch 132] Train Loss=0.9230 | Test Loss=0.8022 | Test Acc=82.15%\n",
      "[S3-Epoch 133] Train Loss=1.0191 | Test Loss=0.7983 | Test Acc=81.98%\n",
      "[S3-Epoch 134] Train Loss=1.0562 | Test Loss=0.8093 | Test Acc=82.00%\n",
      "[S3-Epoch 135] Train Loss=0.9430 | Test Loss=0.8032 | Test Acc=82.02%\n",
      "[S3-Epoch 136] Train Loss=0.9459 | Test Loss=0.8122 | Test Acc=81.91%\n",
      "[S3-Epoch 137] Train Loss=0.8788 | Test Loss=0.8033 | Test Acc=81.96%\n",
      "[S3-Epoch 138] Train Loss=0.9388 | Test Loss=0.8059 | Test Acc=82.00%\n",
      "[S3-Epoch 139] Train Loss=0.9948 | Test Loss=0.8042 | Test Acc=82.00%\n",
      "[S3-Epoch 140] Train Loss=0.9182 | Test Loss=0.8021 | Test Acc=81.76%\n",
      "[S3-Epoch 141] Train Loss=0.9622 | Test Loss=0.8025 | Test Acc=82.10%\n",
      "[S3-Epoch 142] Train Loss=0.9490 | Test Loss=0.7993 | Test Acc=81.84%\n",
      "[S3-Epoch 143] Train Loss=0.9852 | Test Loss=0.8071 | Test Acc=81.76%\n",
      "[S3-Epoch 144] Train Loss=0.9854 | Test Loss=0.8003 | Test Acc=82.26%\n",
      "[S3-Epoch 145] Train Loss=0.9931 | Test Loss=0.7963 | Test Acc=82.08%\n",
      "[S3-Epoch 146] Train Loss=0.9399 | Test Loss=0.7999 | Test Acc=82.00%\n",
      "[S3-Epoch 147] Train Loss=1.0458 | Test Loss=0.8025 | Test Acc=81.95%\n",
      "[S3-Epoch 148] Train Loss=1.0413 | Test Loss=0.8085 | Test Acc=81.98%\n",
      "[S3-Epoch 149] Train Loss=0.9337 | Test Loss=0.8054 | Test Acc=81.95%\n",
      "[S3-Epoch 150] Train Loss=1.0232 | Test Loss=0.8027 | Test Acc=81.88%\n",
      "[S3-Epoch 151] Train Loss=0.9941 | Test Loss=0.8066 | Test Acc=82.00%\n",
      "[S3-Epoch 152] Train Loss=1.0091 | Test Loss=0.7987 | Test Acc=81.86%\n",
      "[S3-Epoch 153] Train Loss=1.0141 | Test Loss=0.8003 | Test Acc=81.83%\n",
      "[S3-Epoch 154] Train Loss=0.9450 | Test Loss=0.7985 | Test Acc=82.03%\n",
      "[S3-Epoch 155] Train Loss=1.0182 | Test Loss=0.8052 | Test Acc=82.05%\n",
      "[S3-Epoch 156] Train Loss=0.9602 | Test Loss=0.8067 | Test Acc=81.81%\n",
      "[S3-Epoch 157] Train Loss=0.9391 | Test Loss=0.8013 | Test Acc=82.07%\n",
      "[S3-Epoch 158] Train Loss=0.7809 | Test Loss=0.7943 | Test Acc=81.95%\n",
      "[S3-Epoch 159] Train Loss=1.0080 | Test Loss=0.7937 | Test Acc=81.98%\n",
      "[S3-Epoch 160] Train Loss=0.8877 | Test Loss=0.7971 | Test Acc=82.15%\n",
      "[S3-Epoch 161] Train Loss=1.0367 | Test Loss=0.7999 | Test Acc=81.91%\n",
      "[S3-Epoch 162] Train Loss=0.9288 | Test Loss=0.8023 | Test Acc=82.15%\n",
      "[S3-Epoch 163] Train Loss=0.9580 | Test Loss=0.7997 | Test Acc=81.95%\n",
      "[S3-Epoch 164] Train Loss=0.8275 | Test Loss=0.7953 | Test Acc=82.05%\n",
      "[S3-Epoch 165] Train Loss=0.9269 | Test Loss=0.7952 | Test Acc=81.90%\n",
      "[S3-Epoch 166] Train Loss=1.0094 | Test Loss=0.7979 | Test Acc=82.05%\n",
      "[S3-Epoch 167] Train Loss=1.0227 | Test Loss=0.7908 | Test Acc=81.84%\n",
      "[S3-Epoch 168] Train Loss=1.0362 | Test Loss=0.7986 | Test Acc=81.93%\n",
      "[S3-Epoch 169] Train Loss=0.8941 | Test Loss=0.8003 | Test Acc=81.76%\n",
      "[S3-Epoch 170] Train Loss=1.0050 | Test Loss=0.8004 | Test Acc=81.91%\n",
      "[S3-Epoch 171] Train Loss=0.9745 | Test Loss=0.7931 | Test Acc=82.10%\n",
      "[S3-Epoch 172] Train Loss=0.9713 | Test Loss=0.8034 | Test Acc=81.84%\n",
      "[S3-Epoch 173] Train Loss=0.9378 | Test Loss=0.7931 | Test Acc=81.90%\n",
      "[S3-Epoch 174] Train Loss=1.0221 | Test Loss=0.7925 | Test Acc=81.90%\n",
      "[S3-Epoch 175] Train Loss=0.9968 | Test Loss=0.7892 | Test Acc=81.95%\n",
      "[S3-Epoch 176] Train Loss=0.8588 | Test Loss=0.7911 | Test Acc=82.00%\n",
      "[S3-Epoch 177] Train Loss=0.8829 | Test Loss=0.7940 | Test Acc=82.00%\n",
      "[S3-Epoch 178] Train Loss=0.9631 | Test Loss=0.8155 | Test Acc=81.90%\n",
      "[S3-Epoch 179] Train Loss=0.9331 | Test Loss=0.7963 | Test Acc=82.02%\n",
      "[S3-Epoch 180] Train Loss=1.0892 | Test Loss=0.8098 | Test Acc=81.74%\n",
      "[S3-Epoch 181] Train Loss=0.9285 | Test Loss=0.8195 | Test Acc=81.86%\n",
      "[S3-Epoch 182] Train Loss=0.9262 | Test Loss=0.8053 | Test Acc=81.95%\n",
      "[S3-Epoch 183] Train Loss=0.8681 | Test Loss=0.7992 | Test Acc=81.95%\n",
      "[S3-Epoch 184] Train Loss=0.8799 | Test Loss=0.7928 | Test Acc=81.88%\n",
      "[S3-Epoch 185] Train Loss=0.8734 | Test Loss=0.7909 | Test Acc=82.07%\n",
      "[S3-Epoch 186] Train Loss=0.9839 | Test Loss=0.7897 | Test Acc=82.05%\n",
      "[S3-Epoch 187] Train Loss=0.8542 | Test Loss=0.7947 | Test Acc=82.08%\n",
      "[S3-Epoch 188] Train Loss=0.8867 | Test Loss=0.7893 | Test Acc=82.14%\n",
      "[S3-Epoch 189] Train Loss=0.7329 | Test Loss=0.7913 | Test Acc=82.08%\n",
      "[S3-Epoch 190] Train Loss=0.9615 | Test Loss=0.7922 | Test Acc=82.08%\n",
      "[S3-Epoch 191] Train Loss=0.9426 | Test Loss=0.7871 | Test Acc=82.14%\n",
      "[S3-Epoch 192] Train Loss=0.9099 | Test Loss=0.7980 | Test Acc=81.81%\n",
      "[S3-Epoch 193] Train Loss=0.9400 | Test Loss=0.7921 | Test Acc=81.95%\n",
      "[S3-Epoch 194] Train Loss=0.9774 | Test Loss=0.7946 | Test Acc=82.26%\n",
      "[S3-Epoch 195] Train Loss=0.9185 | Test Loss=0.8087 | Test Acc=81.91%\n",
      "[S3-Epoch 196] Train Loss=0.9611 | Test Loss=0.7924 | Test Acc=82.03%\n",
      "[S3-Epoch 197] Train Loss=1.0298 | Test Loss=0.7963 | Test Acc=82.07%\n",
      "[S3-Epoch 198] Train Loss=0.9043 | Test Loss=0.7874 | Test Acc=82.26%\n",
      "[S3-Epoch 199] Train Loss=0.9226 | Test Loss=0.7916 | Test Acc=82.08%\n",
      "[S3-Epoch 200] Train Loss=0.9499 | Test Loss=0.7962 | Test Acc=82.19%\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model_fn in [(\"BilinearCNN\", BilinearCNN)]:\n",
    "    model = model_fn().to(DEVICE)\n",
    "    s1, s2, s3 = run_training_3stage(name, model)\n",
    "    df1 = pd.DataFrame(s1, columns=[\"epoch\", \"train_loss\", \"test_loss\", \"test_acc\"])\n",
    "    df1[\"stage\"] = \"Stage 1\"\n",
    "    df1[\"model\"] = name\n",
    "    \n",
    "    df2 = pd.DataFrame(s2, columns=[\"epoch\", \"train_loss\", \"test_loss\", \"test_acc\"])\n",
    "    df2[\"stage\"] = \"Stage 2\"\n",
    "    df2[\"model\"] = name\n",
    "    \n",
    "    df3 = pd.DataFrame(s3, columns=[\"epoch\", \"train_loss\", \"test_loss\", \"test_acc\"])\n",
    "    df3[\"stage\"] = \"Stage 3\"\n",
    "    df3[\"model\"] = name\n",
    "    hist = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7b94aec-f2fd-46a9-b327-01ebd03b788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.to_csv(\"BiCNN_3stg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e93f60-8da0-4364-9888-88125d9298be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
