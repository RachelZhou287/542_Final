{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efd20df-9795-4e6c-a7b2-576b78bc39f1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648abb97-4b7e-4e18-ae6a-982db4a7240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "import os, time, random, numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a957d8e-0e24-4aa4-ae86-972ca0e0df8a",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69981344-775c-4803-bbb1-9653ade4a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_ROOT = \"data/CUB_200_2011\"\n",
    "NUM_CLASSES = 200\n",
    "INIT_LR = 1e-4\n",
    "NEW_LR = 1e-6\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "STEP_SIZE = 7\n",
    "GAMMA = 0.1\n",
    "EPOCHS_STAGE1 = 7\n",
    "EPOCHS_STAGE2 = 150\n",
    "SEED = 87\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ea54b-8b86-41c3-97c8-a99313b1f599",
   "metadata": {},
   "source": [
    "# Datset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba79ce28-2a0f-4483-af61-32b93271fce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5994 training and 5794 testing images.\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        img_txt = os.path.join(root, \"images.txt\")\n",
    "        label_txt = os.path.join(root, \"image_class_labels.txt\")\n",
    "        split_txt = os.path.join(root, \"train_test_split.txt\")\n",
    "\n",
    "        with open(img_txt) as f:\n",
    "            imgs = [x.strip().split(\" \") for x in f.readlines()]\n",
    "        with open(label_txt) as f:\n",
    "            labels = [int(x.strip().split(\" \")[1]) - 1 for x in f.readlines()]\n",
    "        with open(split_txt) as f:\n",
    "            split = [int(x.strip().split(\" \")[1]) for x in f.readlines()]\n",
    "\n",
    "        self.samples = []\n",
    "        for (img_id, img_path), label, is_train in zip(imgs, labels, split):\n",
    "            if (train and is_train == 1) or (not train and is_train == 0):\n",
    "                self.samples.append((os.path.join(root, \"images\", img_path), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "train_set = CUBDataset(DATA_ROOT, train=True, transform=train_transforms)\n",
    "test_set = CUBDataset(DATA_ROOT, train=False, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=10,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(train_set)} training and {len(test_set)} testing images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c139d4-d34a-4cc7-a2ca-13e1e4b296c4",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2ecea-3162-4890-b5e1-40c46d9359b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MixUp Augmentation\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        batch_size = labels.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return 100.0 * correct / total, total_loss / total\n",
    "\n",
    "# Train\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        mixed_imgs, y_a, y_b, lam = mixup_data(images, labels, alpha=0.2, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixed_imgs)\n",
    "        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            raw_outputs = model(images)\n",
    "            preds = raw_outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41afba-a922-4c4e-8c6d-a75c16461944",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e536df83-cf99-422b-a262-148dbe953865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage 1 ===\n",
      "[S1-Epoch 01] Train Loss=0.1640 | Train Acc=2.49% | Test Loss=4.9199 | Test Acc=4.87% | LR=1.00e-04 | Time=11.06s\n",
      "[S1-Epoch 02] Train Loss=0.1525 | Train Acc=11.71% | Test Loss=4.5216 | Test Acc=21.14% | LR=1.00e-04 | Time=10.84s\n",
      "[S1-Epoch 03] Train Loss=0.1433 | Train Acc=22.86% | Test Loss=4.2164 | Test Acc=28.96% | LR=1.00e-04 | Time=10.79s\n",
      "[S1-Epoch 04] Train Loss=0.1359 | Train Acc=30.10% | Test Loss=3.9412 | Test Acc=32.41% | LR=1.00e-04 | Time=10.94s\n",
      "[S1-Epoch 05] Train Loss=0.1284 | Train Acc=36.17% | Test Loss=3.6771 | Test Acc=38.61% | LR=1.00e-04 | Time=11.01s\n",
      "[S1-Epoch 06] Train Loss=0.1241 | Train Acc=39.97% | Test Loss=3.4888 | Test Acc=40.87% | LR=1.00e-04 | Time=10.98s\n",
      "[S1-Epoch 07] Train Loss=0.1174 | Train Acc=43.29% | Test Loss=3.3133 | Test Acc=42.51% | LR=1.00e-05 | Time=10.89s\n",
      "=== Stage 2===\n",
      "[S2-Epoch 001] Train Loss=0.1097 | Train Acc=49.13% | Test Loss=3.0465 | Test Acc=47.74% | LR=1.00e-06 | Time=20.22s\n",
      "[S2-Epoch 002] Train Loss=0.1043 | Train Acc=52.07% | Test Loss=2.8711 | Test Acc=49.67% | LR=1.00e-06 | Time=20.62s\n",
      "[S2-Epoch 003] Train Loss=0.1013 | Train Acc=53.12% | Test Loss=2.7241 | Test Acc=51.04% | LR=1.00e-06 | Time=19.59s\n",
      "[S2-Epoch 004] Train Loss=0.0984 | Train Acc=55.51% | Test Loss=2.6467 | Test Acc=52.33% | LR=1.00e-06 | Time=19.30s\n",
      "[S2-Epoch 005] Train Loss=0.0963 | Train Acc=55.12% | Test Loss=2.5851 | Test Acc=53.47% | LR=1.00e-06 | Time=20.16s\n",
      "[S2-Epoch 006] Train Loss=0.0926 | Train Acc=56.87% | Test Loss=2.4919 | Test Acc=54.52% | LR=1.00e-06 | Time=19.54s\n",
      "[S2-Epoch 007] Train Loss=0.0911 | Train Acc=57.96% | Test Loss=2.4319 | Test Acc=56.02% | LR=1.00e-06 | Time=19.39s\n",
      "[S2-Epoch 008] Train Loss=0.0943 | Train Acc=59.18% | Test Loss=2.3934 | Test Acc=56.80% | LR=1.00e-06 | Time=19.48s\n",
      "[S2-Epoch 009] Train Loss=0.0926 | Train Acc=60.31% | Test Loss=2.3441 | Test Acc=57.96% | LR=1.00e-06 | Time=18.96s\n",
      "[S2-Epoch 010] Train Loss=0.0908 | Train Acc=61.49% | Test Loss=2.2777 | Test Acc=59.86% | LR=1.00e-06 | Time=19.79s\n",
      "[S2-Epoch 011] Train Loss=0.0895 | Train Acc=62.20% | Test Loss=2.2556 | Test Acc=59.51% | LR=1.00e-06 | Time=18.59s\n",
      "[S2-Epoch 012] Train Loss=0.0872 | Train Acc=62.28% | Test Loss=2.1935 | Test Acc=60.53% | LR=1.00e-06 | Time=20.02s\n",
      "[S2-Epoch 013] Train Loss=0.0869 | Train Acc=62.85% | Test Loss=2.1752 | Test Acc=61.03% | LR=1.00e-06 | Time=19.62s\n",
      "[S2-Epoch 014] Train Loss=0.0862 | Train Acc=64.23% | Test Loss=2.1144 | Test Acc=61.77% | LR=1.00e-06 | Time=19.02s\n",
      "[S2-Epoch 015] Train Loss=0.0845 | Train Acc=64.80% | Test Loss=2.1348 | Test Acc=61.41% | LR=1.00e-06 | Time=19.59s\n",
      "[S2-Epoch 016] Train Loss=0.0835 | Train Acc=65.00% | Test Loss=2.1143 | Test Acc=62.46% | LR=1.00e-06 | Time=20.02s\n",
      "[S2-Epoch 017] Train Loss=0.0831 | Train Acc=66.30% | Test Loss=2.0255 | Test Acc=63.01% | LR=1.00e-06 | Time=20.16s\n",
      "[S2-Epoch 018] Train Loss=0.0822 | Train Acc=66.42% | Test Loss=2.0423 | Test Acc=64.10% | LR=1.00e-06 | Time=19.55s\n",
      "[S2-Epoch 019] Train Loss=0.0825 | Train Acc=66.93% | Test Loss=1.9893 | Test Acc=64.65% | LR=1.00e-06 | Time=19.70s\n",
      "[S2-Epoch 020] Train Loss=0.0807 | Train Acc=67.28% | Test Loss=1.9864 | Test Acc=64.77% | LR=1.00e-06 | Time=19.72s\n",
      "[S2-Epoch 021] Train Loss=0.0821 | Train Acc=68.32% | Test Loss=1.9287 | Test Acc=64.88% | LR=1.00e-06 | Time=20.65s\n",
      "[S2-Epoch 022] Train Loss=0.0775 | Train Acc=68.94% | Test Loss=1.9214 | Test Acc=65.72% | LR=1.00e-06 | Time=20.46s\n",
      "[S2-Epoch 023] Train Loss=0.0785 | Train Acc=69.24% | Test Loss=1.9028 | Test Acc=66.00% | LR=1.00e-06 | Time=20.20s\n",
      "[S2-Epoch 024] Train Loss=0.0774 | Train Acc=69.64% | Test Loss=1.8632 | Test Acc=66.52% | LR=1.00e-06 | Time=20.14s\n",
      "[S2-Epoch 025] Train Loss=0.0781 | Train Acc=70.27% | Test Loss=1.8561 | Test Acc=67.41% | LR=1.00e-06 | Time=20.64s\n",
      "[S2-Epoch 026] Train Loss=0.0793 | Train Acc=69.99% | Test Loss=1.8206 | Test Acc=67.50% | LR=1.00e-06 | Time=20.43s\n",
      "[S2-Epoch 027] Train Loss=0.0800 | Train Acc=71.09% | Test Loss=1.8009 | Test Acc=67.26% | LR=1.00e-06 | Time=19.53s\n",
      "[S2-Epoch 028] Train Loss=0.0726 | Train Acc=71.96% | Test Loss=1.8085 | Test Acc=67.24% | LR=1.00e-06 | Time=19.79s\n",
      "[S2-Epoch 029] Train Loss=0.0758 | Train Acc=72.12% | Test Loss=1.7590 | Test Acc=68.55% | LR=1.00e-06 | Time=19.99s\n",
      "[S2-Epoch 030] Train Loss=0.0748 | Train Acc=71.42% | Test Loss=1.7442 | Test Acc=68.83% | LR=1.00e-06 | Time=19.38s\n",
      "[S2-Epoch 031] Train Loss=0.0735 | Train Acc=73.01% | Test Loss=1.7418 | Test Acc=68.43% | LR=1.00e-06 | Time=19.09s\n",
      "[S2-Epoch 032] Train Loss=0.0726 | Train Acc=72.69% | Test Loss=1.6972 | Test Acc=68.95% | LR=1.00e-06 | Time=19.32s\n",
      "[S2-Epoch 033] Train Loss=0.0725 | Train Acc=72.96% | Test Loss=1.7049 | Test Acc=69.05% | LR=1.00e-06 | Time=19.05s\n",
      "[S2-Epoch 034] Train Loss=0.0716 | Train Acc=73.42% | Test Loss=1.7020 | Test Acc=69.33% | LR=1.00e-06 | Time=18.72s\n",
      "[S2-Epoch 035] Train Loss=0.0723 | Train Acc=74.01% | Test Loss=1.6833 | Test Acc=69.81% | LR=1.00e-06 | Time=19.39s\n",
      "[S2-Epoch 036] Train Loss=0.0713 | Train Acc=73.69% | Test Loss=1.6885 | Test Acc=69.85% | LR=1.00e-06 | Time=19.43s\n",
      "[S2-Epoch 037] Train Loss=0.0686 | Train Acc=75.26% | Test Loss=1.6423 | Test Acc=70.21% | LR=1.00e-06 | Time=19.66s\n",
      "[S2-Epoch 038] Train Loss=0.0697 | Train Acc=74.61% | Test Loss=1.6126 | Test Acc=71.07% | LR=1.00e-06 | Time=19.25s\n",
      "[S2-Epoch 039] Train Loss=0.0690 | Train Acc=74.82% | Test Loss=1.6097 | Test Acc=70.68% | LR=1.00e-06 | Time=20.13s\n",
      "[S2-Epoch 040] Train Loss=0.0678 | Train Acc=76.13% | Test Loss=1.5710 | Test Acc=70.80% | LR=1.00e-06 | Time=18.67s\n",
      "[S2-Epoch 041] Train Loss=0.0686 | Train Acc=75.83% | Test Loss=1.5864 | Test Acc=71.19% | LR=1.00e-06 | Time=19.91s\n",
      "[S2-Epoch 042] Train Loss=0.0683 | Train Acc=77.56% | Test Loss=1.5618 | Test Acc=71.87% | LR=1.00e-06 | Time=20.74s\n",
      "[S2-Epoch 043] Train Loss=0.0705 | Train Acc=76.99% | Test Loss=1.5304 | Test Acc=71.85% | LR=1.00e-06 | Time=20.75s\n",
      "[S2-Epoch 044] Train Loss=0.0695 | Train Acc=77.41% | Test Loss=1.5501 | Test Acc=72.07% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 045] Train Loss=0.0674 | Train Acc=77.38% | Test Loss=1.5333 | Test Acc=71.52% | LR=1.00e-06 | Time=21.01s\n",
      "[S2-Epoch 046] Train Loss=0.0677 | Train Acc=77.61% | Test Loss=1.5068 | Test Acc=72.73% | LR=1.00e-06 | Time=20.30s\n",
      "[S2-Epoch 047] Train Loss=0.0665 | Train Acc=77.51% | Test Loss=1.4833 | Test Acc=72.97% | LR=1.00e-06 | Time=20.13s\n",
      "[S2-Epoch 048] Train Loss=0.0675 | Train Acc=77.68% | Test Loss=1.5044 | Test Acc=72.49% | LR=1.00e-06 | Time=20.72s\n",
      "[S2-Epoch 049] Train Loss=0.0653 | Train Acc=78.45% | Test Loss=1.4675 | Test Acc=73.27% | LR=1.00e-06 | Time=20.39s\n",
      "[S2-Epoch 050] Train Loss=0.0619 | Train Acc=78.90% | Test Loss=1.4279 | Test Acc=73.46% | LR=1.00e-06 | Time=20.24s\n",
      "[S2-Epoch 051] Train Loss=0.0628 | Train Acc=79.15% | Test Loss=1.4535 | Test Acc=73.28% | LR=1.00e-06 | Time=19.76s\n",
      "[S2-Epoch 052] Train Loss=0.0657 | Train Acc=79.80% | Test Loss=1.4406 | Test Acc=73.28% | LR=1.00e-06 | Time=20.81s\n",
      "[S2-Epoch 053] Train Loss=0.0636 | Train Acc=79.61% | Test Loss=1.4220 | Test Acc=73.08% | LR=1.00e-06 | Time=20.53s\n",
      "[S2-Epoch 054] Train Loss=0.0658 | Train Acc=79.65% | Test Loss=1.4123 | Test Acc=73.68% | LR=1.00e-06 | Time=20.54s\n",
      "[S2-Epoch 055] Train Loss=0.0631 | Train Acc=80.16% | Test Loss=1.4280 | Test Acc=73.47% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 056] Train Loss=0.0651 | Train Acc=79.90% | Test Loss=1.3889 | Test Acc=73.85% | LR=1.00e-06 | Time=20.46s\n",
      "[S2-Epoch 057] Train Loss=0.0662 | Train Acc=80.98% | Test Loss=1.3819 | Test Acc=74.28% | LR=1.00e-06 | Time=20.93s\n",
      "[S2-Epoch 058] Train Loss=0.0655 | Train Acc=80.15% | Test Loss=1.3838 | Test Acc=73.97% | LR=1.00e-06 | Time=20.84s\n",
      "[S2-Epoch 059] Train Loss=0.0617 | Train Acc=80.28% | Test Loss=1.3702 | Test Acc=74.54% | LR=1.00e-06 | Time=20.66s\n",
      "[S2-Epoch 060] Train Loss=0.0602 | Train Acc=80.98% | Test Loss=1.3574 | Test Acc=74.44% | LR=1.00e-06 | Time=20.88s\n",
      "[S2-Epoch 061] Train Loss=0.0573 | Train Acc=81.33% | Test Loss=1.3545 | Test Acc=74.89% | LR=1.00e-06 | Time=20.67s\n",
      "[S2-Epoch 062] Train Loss=0.0558 | Train Acc=81.70% | Test Loss=1.3338 | Test Acc=75.09% | LR=1.00e-06 | Time=20.31s\n",
      "[S2-Epoch 063] Train Loss=0.0595 | Train Acc=81.83% | Test Loss=1.3334 | Test Acc=75.09% | LR=1.00e-06 | Time=20.81s\n",
      "[S2-Epoch 064] Train Loss=0.0612 | Train Acc=82.12% | Test Loss=1.3125 | Test Acc=75.51% | LR=1.00e-06 | Time=19.49s\n",
      "[S2-Epoch 065] Train Loss=0.0586 | Train Acc=82.00% | Test Loss=1.3122 | Test Acc=75.23% | LR=1.00e-06 | Time=20.50s\n",
      "[S2-Epoch 066] Train Loss=0.0561 | Train Acc=82.22% | Test Loss=1.3261 | Test Acc=74.92% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 067] Train Loss=0.0554 | Train Acc=82.68% | Test Loss=1.2898 | Test Acc=75.89% | LR=1.00e-06 | Time=20.13s\n",
      "[S2-Epoch 068] Train Loss=0.0600 | Train Acc=83.58% | Test Loss=1.3062 | Test Acc=75.47% | LR=1.00e-06 | Time=20.92s\n",
      "[S2-Epoch 069] Train Loss=0.0540 | Train Acc=83.30% | Test Loss=1.2552 | Test Acc=75.89% | LR=1.00e-06 | Time=18.58s\n",
      "[S2-Epoch 070] Train Loss=0.0558 | Train Acc=83.22% | Test Loss=1.2789 | Test Acc=75.77% | LR=1.00e-06 | Time=18.75s\n",
      "[S2-Epoch 071] Train Loss=0.0542 | Train Acc=83.22% | Test Loss=1.2316 | Test Acc=76.22% | LR=1.00e-06 | Time=20.20s\n",
      "[S2-Epoch 072] Train Loss=0.0583 | Train Acc=83.57% | Test Loss=1.2426 | Test Acc=76.89% | LR=1.00e-06 | Time=20.38s\n",
      "[S2-Epoch 073] Train Loss=0.0550 | Train Acc=83.55% | Test Loss=1.2308 | Test Acc=76.41% | LR=1.00e-06 | Time=20.55s\n",
      "[S2-Epoch 074] Train Loss=0.0576 | Train Acc=84.12% | Test Loss=1.2090 | Test Acc=76.46% | LR=1.00e-06 | Time=20.21s\n",
      "[S2-Epoch 075] Train Loss=0.0535 | Train Acc=84.08% | Test Loss=1.2063 | Test Acc=77.20% | LR=1.00e-06 | Time=20.59s\n",
      "[S2-Epoch 076] Train Loss=0.0567 | Train Acc=84.52% | Test Loss=1.2114 | Test Acc=76.63% | LR=1.00e-06 | Time=20.46s\n",
      "[S2-Epoch 077] Train Loss=0.0550 | Train Acc=84.57% | Test Loss=1.2078 | Test Acc=76.46% | LR=1.00e-06 | Time=20.68s\n",
      "[S2-Epoch 078] Train Loss=0.0534 | Train Acc=84.52% | Test Loss=1.1890 | Test Acc=76.72% | LR=1.00e-06 | Time=20.64s\n",
      "[S2-Epoch 079] Train Loss=0.0551 | Train Acc=84.58% | Test Loss=1.2253 | Test Acc=76.49% | LR=1.00e-06 | Time=20.06s\n",
      "[S2-Epoch 080] Train Loss=0.0586 | Train Acc=85.25% | Test Loss=1.1758 | Test Acc=77.20% | LR=1.00e-06 | Time=20.24s\n",
      "[S2-Epoch 081] Train Loss=0.0546 | Train Acc=84.67% | Test Loss=1.1792 | Test Acc=77.17% | LR=1.00e-06 | Time=20.85s\n",
      "[S2-Epoch 082] Train Loss=0.0557 | Train Acc=85.64% | Test Loss=1.1902 | Test Acc=77.08% | LR=1.00e-06 | Time=20.78s\n",
      "[S2-Epoch 083] Train Loss=0.0535 | Train Acc=85.37% | Test Loss=1.1798 | Test Acc=77.20% | LR=1.00e-06 | Time=20.50s\n",
      "[S2-Epoch 084] Train Loss=0.0555 | Train Acc=85.62% | Test Loss=1.1583 | Test Acc=77.44% | LR=1.00e-06 | Time=20.67s\n",
      "[S2-Epoch 085] Train Loss=0.0516 | Train Acc=86.04% | Test Loss=1.1525 | Test Acc=77.51% | LR=1.00e-06 | Time=20.47s\n",
      "[S2-Epoch 086] Train Loss=0.0554 | Train Acc=85.70% | Test Loss=1.1429 | Test Acc=77.34% | LR=1.00e-06 | Time=20.79s\n",
      "[S2-Epoch 087] Train Loss=0.0527 | Train Acc=86.10% | Test Loss=1.1601 | Test Acc=77.53% | LR=1.00e-06 | Time=20.57s\n",
      "[S2-Epoch 088] Train Loss=0.0545 | Train Acc=86.14% | Test Loss=1.1460 | Test Acc=77.25% | LR=1.00e-06 | Time=20.85s\n",
      "[S2-Epoch 089] Train Loss=0.0504 | Train Acc=86.10% | Test Loss=1.1198 | Test Acc=77.74% | LR=1.00e-06 | Time=20.59s\n",
      "[S2-Epoch 090] Train Loss=0.0509 | Train Acc=86.40% | Test Loss=1.1192 | Test Acc=78.08% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 091] Train Loss=0.0528 | Train Acc=86.62% | Test Loss=1.1333 | Test Acc=77.93% | LR=1.00e-06 | Time=20.13s\n",
      "[S2-Epoch 092] Train Loss=0.0515 | Train Acc=86.55% | Test Loss=1.1114 | Test Acc=77.67% | LR=1.00e-06 | Time=20.84s\n",
      "[S2-Epoch 093] Train Loss=0.0517 | Train Acc=87.05% | Test Loss=1.1133 | Test Acc=78.24% | LR=1.00e-06 | Time=20.18s\n",
      "[S2-Epoch 094] Train Loss=0.0504 | Train Acc=87.54% | Test Loss=1.0820 | Test Acc=77.74% | LR=1.00e-06 | Time=20.72s\n",
      "[S2-Epoch 095] Train Loss=0.0500 | Train Acc=87.20% | Test Loss=1.0921 | Test Acc=78.01% | LR=1.00e-06 | Time=20.63s\n",
      "[S2-Epoch 096] Train Loss=0.0493 | Train Acc=87.79% | Test Loss=1.1046 | Test Acc=78.12% | LR=1.00e-06 | Time=20.31s\n",
      "[S2-Epoch 097] Train Loss=0.0487 | Train Acc=87.95% | Test Loss=1.0875 | Test Acc=78.24% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 098] Train Loss=0.0499 | Train Acc=87.95% | Test Loss=1.0770 | Test Acc=78.58% | LR=1.00e-06 | Time=20.59s\n",
      "[S2-Epoch 099] Train Loss=0.0494 | Train Acc=87.64% | Test Loss=1.0589 | Test Acc=78.56% | LR=1.00e-06 | Time=20.58s\n",
      "[S2-Epoch 100] Train Loss=0.0496 | Train Acc=87.65% | Test Loss=1.0592 | Test Acc=78.53% | LR=1.00e-06 | Time=20.53s\n",
      "[S2-Epoch 101] Train Loss=0.0498 | Train Acc=88.56% | Test Loss=1.0868 | Test Acc=78.39% | LR=1.00e-06 | Time=20.63s\n",
      "[S2-Epoch 102] Train Loss=0.0496 | Train Acc=88.36% | Test Loss=1.0655 | Test Acc=78.46% | LR=1.00e-06 | Time=20.67s\n",
      "[S2-Epoch 103] Train Loss=0.0534 | Train Acc=87.79% | Test Loss=1.0375 | Test Acc=78.62% | LR=1.00e-06 | Time=20.60s\n",
      "[S2-Epoch 104] Train Loss=0.0501 | Train Acc=88.39% | Test Loss=1.0450 | Test Acc=78.79% | LR=1.00e-06 | Time=20.36s\n",
      "[S2-Epoch 105] Train Loss=0.0460 | Train Acc=88.57% | Test Loss=1.0436 | Test Acc=78.86% | LR=1.00e-06 | Time=20.65s\n",
      "[S2-Epoch 106] Train Loss=0.0467 | Train Acc=88.87% | Test Loss=1.0389 | Test Acc=78.77% | LR=1.00e-06 | Time=20.43s\n",
      "[S2-Epoch 107] Train Loss=0.0476 | Train Acc=88.84% | Test Loss=1.0546 | Test Acc=78.87% | LR=1.00e-06 | Time=20.70s\n",
      "[S2-Epoch 108] Train Loss=0.0457 | Train Acc=88.86% | Test Loss=1.0391 | Test Acc=78.48% | LR=1.00e-06 | Time=20.95s\n",
      "[S2-Epoch 109] Train Loss=0.0492 | Train Acc=89.09% | Test Loss=1.0263 | Test Acc=79.10% | LR=1.00e-06 | Time=19.72s\n",
      "[S2-Epoch 110] Train Loss=0.0506 | Train Acc=88.82% | Test Loss=1.0148 | Test Acc=79.25% | LR=1.00e-06 | Time=20.71s\n",
      "[S2-Epoch 111] Train Loss=0.0460 | Train Acc=89.16% | Test Loss=1.0094 | Test Acc=78.84% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 112] Train Loss=0.0463 | Train Acc=88.89% | Test Loss=1.0148 | Test Acc=79.25% | LR=1.00e-06 | Time=20.87s\n",
      "[S2-Epoch 113] Train Loss=0.0444 | Train Acc=89.94% | Test Loss=1.0022 | Test Acc=79.31% | LR=1.00e-06 | Time=20.85s\n",
      "[S2-Epoch 114] Train Loss=0.0453 | Train Acc=89.47% | Test Loss=0.9844 | Test Acc=79.38% | LR=1.00e-06 | Time=20.72s\n",
      "[S2-Epoch 115] Train Loss=0.0444 | Train Acc=89.52% | Test Loss=0.9801 | Test Acc=78.84% | LR=1.00e-06 | Time=20.75s\n",
      "[S2-Epoch 116] Train Loss=0.0467 | Train Acc=90.41% | Test Loss=0.9801 | Test Acc=79.46% | LR=1.00e-06 | Time=20.62s\n",
      "[S2-Epoch 117] Train Loss=0.0455 | Train Acc=89.84% | Test Loss=0.9897 | Test Acc=79.82% | LR=1.00e-06 | Time=20.72s\n",
      "[S2-Epoch 118] Train Loss=0.0478 | Train Acc=89.89% | Test Loss=1.0069 | Test Acc=79.08% | LR=1.00e-06 | Time=20.44s\n",
      "[S2-Epoch 119] Train Loss=0.0405 | Train Acc=90.57% | Test Loss=0.9761 | Test Acc=79.84% | LR=1.00e-06 | Time=20.43s\n",
      "[S2-Epoch 120] Train Loss=0.0474 | Train Acc=90.06% | Test Loss=0.9799 | Test Acc=79.75% | LR=1.00e-06 | Time=20.57s\n",
      "[S2-Epoch 121] Train Loss=0.0453 | Train Acc=90.64% | Test Loss=0.9720 | Test Acc=79.43% | LR=1.00e-06 | Time=20.72s\n",
      "[S2-Epoch 122] Train Loss=0.0448 | Train Acc=90.82% | Test Loss=0.9712 | Test Acc=79.63% | LR=1.00e-06 | Time=20.87s\n",
      "[S2-Epoch 123] Train Loss=0.0444 | Train Acc=90.87% | Test Loss=0.9733 | Test Acc=79.51% | LR=1.00e-06 | Time=20.69s\n",
      "[S2-Epoch 124] Train Loss=0.0472 | Train Acc=89.97% | Test Loss=0.9572 | Test Acc=79.82% | LR=1.00e-06 | Time=20.35s\n",
      "[S2-Epoch 125] Train Loss=0.0462 | Train Acc=90.97% | Test Loss=0.9775 | Test Acc=79.43% | LR=1.00e-06 | Time=20.80s\n",
      "[S2-Epoch 126] Train Loss=0.0461 | Train Acc=90.74% | Test Loss=0.9698 | Test Acc=79.72% | LR=1.00e-06 | Time=20.47s\n",
      "[S2-Epoch 127] Train Loss=0.0467 | Train Acc=91.26% | Test Loss=0.9780 | Test Acc=79.67% | LR=1.00e-06 | Time=20.78s\n",
      "[S2-Epoch 128] Train Loss=0.0446 | Train Acc=91.06% | Test Loss=0.9653 | Test Acc=79.98% | LR=1.00e-06 | Time=20.49s\n",
      "[S2-Epoch 129] Train Loss=0.0436 | Train Acc=90.84% | Test Loss=0.9393 | Test Acc=79.70% | LR=1.00e-06 | Time=20.63s\n",
      "[S2-Epoch 130] Train Loss=0.0427 | Train Acc=91.06% | Test Loss=0.9582 | Test Acc=79.43% | LR=1.00e-06 | Time=20.89s\n",
      "[S2-Epoch 131] Train Loss=0.0389 | Train Acc=91.12% | Test Loss=0.9330 | Test Acc=79.70% | LR=1.00e-06 | Time=20.66s\n",
      "[S2-Epoch 132] Train Loss=0.0433 | Train Acc=91.29% | Test Loss=0.9525 | Test Acc=79.50% | LR=1.00e-06 | Time=20.84s\n",
      "[S2-Epoch 133] Train Loss=0.0434 | Train Acc=91.32% | Test Loss=0.9348 | Test Acc=79.94% | LR=1.00e-06 | Time=20.52s\n",
      "[S2-Epoch 134] Train Loss=0.0447 | Train Acc=91.61% | Test Loss=0.9258 | Test Acc=79.98% | LR=1.00e-06 | Time=20.11s\n",
      "[S2-Epoch 135] Train Loss=0.0437 | Train Acc=92.01% | Test Loss=0.9285 | Test Acc=79.96% | LR=1.00e-06 | Time=20.59s\n",
      "[S2-Epoch 136] Train Loss=0.0435 | Train Acc=91.51% | Test Loss=0.9629 | Test Acc=79.94% | LR=1.00e-06 | Time=20.60s\n",
      "[S2-Epoch 137] Train Loss=0.0434 | Train Acc=91.61% | Test Loss=0.9423 | Test Acc=79.88% | LR=1.00e-06 | Time=20.71s\n",
      "[S2-Epoch 138] Train Loss=0.0404 | Train Acc=91.78% | Test Loss=0.9117 | Test Acc=79.88% | LR=1.00e-06 | Time=20.21s\n",
      "[S2-Epoch 139] Train Loss=0.0453 | Train Acc=92.18% | Test Loss=0.9232 | Test Acc=79.84% | LR=1.00e-06 | Time=20.69s\n",
      "[S2-Epoch 140] Train Loss=0.0422 | Train Acc=91.99% | Test Loss=0.9341 | Test Acc=80.01% | LR=1.00e-06 | Time=20.96s\n",
      "[S2-Epoch 141] Train Loss=0.0367 | Train Acc=91.96% | Test Loss=0.8873 | Test Acc=80.38% | LR=1.00e-06 | Time=20.65s\n",
      "[S2-Epoch 142] Train Loss=0.0397 | Train Acc=92.13% | Test Loss=0.9012 | Test Acc=80.17% | LR=1.00e-06 | Time=20.66s\n",
      "[S2-Epoch 143] Train Loss=0.0427 | Train Acc=92.64% | Test Loss=0.9213 | Test Acc=80.19% | LR=1.00e-06 | Time=20.49s\n",
      "[S2-Epoch 144] Train Loss=0.0407 | Train Acc=92.54% | Test Loss=0.8687 | Test Acc=80.22% | LR=1.00e-06 | Time=19.78s\n",
      "[S2-Epoch 145] Train Loss=0.0400 | Train Acc=92.48% | Test Loss=0.8962 | Test Acc=80.29% | LR=1.00e-06 | Time=20.89s\n",
      "[S2-Epoch 146] Train Loss=0.0449 | Train Acc=92.43% | Test Loss=0.8856 | Test Acc=80.53% | LR=1.00e-06 | Time=20.77s\n",
      "[S2-Epoch 147] Train Loss=0.0392 | Train Acc=92.48% | Test Loss=0.9004 | Test Acc=80.13% | LR=1.00e-06 | Time=20.67s\n",
      "[S2-Epoch 148] Train Loss=0.0403 | Train Acc=92.99% | Test Loss=0.8909 | Test Acc=80.22% | LR=1.00e-06 | Time=20.82s\n",
      "[S2-Epoch 149] Train Loss=0.0407 | Train Acc=93.18% | Test Loss=0.9005 | Test Acc=80.38% | LR=1.00e-06 | Time=20.73s\n",
      "[S2-Epoch 150] Train Loss=0.0406 | Train Acc=92.69% | Test Loss=0.8903 | Test Acc=80.51% | LR=1.00e-07 | Time=20.70s\n",
      "Final Best Acc = 80.53%\n"
     ]
    }
   ],
   "source": [
    "# Stage 1\n",
    "print(\"=== Stage 1 ===\")\n",
    "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.fc.parameters(), lr=INIT_LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "best_acc_stage1 = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE1):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train_model(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    test_losses.append(te_loss)\n",
    "    train_accuracies.append(tr_acc)\n",
    "    test_accuracies.append(te_acc)\n",
    "\n",
    "    print(f\"[S1-Epoch {epoch+1:02d}] Train Loss={tr_loss:.4f} | Train Acc={tr_acc:.2f}% | \"\n",
    "          f\"Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}% | \"\n",
    "          f\"LR={optimizer.param_groups[0]['lr']:.2e} | Time={time.time()-start:.2f}s\")\n",
    "\n",
    "    if te_acc > best_acc_stage1:\n",
    "        best_acc_stage1 = te_acc\n",
    "        torch.save(model.state_dict(), \"best_resnet_stage1.pt\")\n",
    "\n",
    "# Stage 2\n",
    "print(\"=== Stage 2===\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=NEW_LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=EPOCHS_STAGE2, gamma=0.1)\n",
    "\n",
    "best_acc_stage2 = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE2):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train_model(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    te_acc, te_loss = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    test_losses.append(te_loss)\n",
    "    train_accuracies.append(tr_acc)\n",
    "    test_accuracies.append(te_acc)\n",
    "\n",
    "    print(f\"[S2-Epoch {epoch+1:03d}] Train Loss={tr_loss:.4f} | Train Acc={tr_acc:.2f}% | \"\n",
    "          f\"Test Loss={te_loss:.4f} | Test Acc={te_acc:.2f}% | \"\n",
    "          f\"LR={optimizer.param_groups[0]['lr']:.2e} | Time={time.time()-start:.2f}s\")\n",
    "\n",
    "    if te_acc > best_acc_stage2:\n",
    "        best_acc_stage2 = te_acc\n",
    "        torch.save(model.state_dict(), \"best_resnet_finetuned.pt\")\n",
    "\n",
    "print(f\"Final Best Acc = {best_acc_stage2:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
