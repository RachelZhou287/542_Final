{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelZhou287/542_Final/blob/main/BNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "# ! pip install pyro-ppl\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Categorical\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "import pyro.poutine as poutine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxrFjTbgvFeY",
        "outputId": "98994010-f201-4fc3-8725-9fea0f0093c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23.0+cu126\n",
            "Collecting pyro-ppl\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (3.4.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->pyro-ppl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.3)\n",
            "Downloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test integrity first\n",
        "!tar -tzf /content/CUB_200_2011.tgz > /dev/null\n"
      ],
      "metadata": {
        "id": "l8wFtKPDDTgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract safely\n",
        "!mkdir -p /content/data\n",
        "!tar -xzf /content/CUB_200_2011.tgz -C /content/data/"
      ],
      "metadata": {
        "id": "ZO8rA1WmD0UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm\n",
        "!ls /content/data/CUB_200_2011 | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKHEFjlYk5H9",
        "outputId": "5331f71a-3838-4ffc-ea47-aa377ed05ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attributes\n",
            "bounding_boxes.txt\n",
            "classes.txt\n",
            "image_class_labels.txt\n",
            "images\n",
            "images.txt\n",
            "parts\n",
            "README\n",
            "train_test_split.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "P7CaUDjsEHMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CUBDataset(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "\n",
        "        img_files = pd.read_csv(os.path.join(root, \"images.txt\"), sep=\" \", names=[\"img_id\", \"filepath\"])\n",
        "        labels = pd.read_csv(os.path.join(root, \"image_class_labels.txt\"), sep=\" \", names=[\"img_id\", \"target\"])\n",
        "        split = pd.read_csv(os.path.join(root, \"train_test_split.txt\"), sep=\" \", names=[\"img_id\", \"is_training_img\"])\n",
        "        df = img_files.merge(labels, on=\"img_id\").merge(split, on=\"img_id\")\n",
        "        df = df[df[\"is_training_img\"] == int(train)]\n",
        "\n",
        "        self.paths = df[\"filepath\"].values\n",
        "        self.targets = df[\"target\"].values - 1  # 0-indexed\n",
        "\n",
        "    def __len__(self):  return len(self.paths) # number of samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, \"images\", self.paths[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.targets[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# === Transforms + loaders ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "data_root = \"/content/data/CUB_200_2011\"\n",
        "train_data = CUBDataset(data_root, train=True, transform=transform)\n",
        "test_data  = CUBDataset(data_root, train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2) #batch size: group 32 images per training step\n",
        "test_loader  = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✅ Loaded {len(train_data)} training and {len(test_data)} testing images.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2hakI8KEDoV",
        "outputId": "d758798f-36da-40b6-ef69-ebd9cecad6f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 5994 training and 5794 testing images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN Structure"
      ],
      "metadata": {
        "id": "TbmRVx7fGMg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten 3×128×128 RGB images → [batch, 49152]\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.out(x)  # raw logits (no softmax)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "input_size = 3 * 64 * 64\n",
        "net = NN(input_size=input_size, hidden_size=128, output_size=200)"
      ],
      "metadata": {
        "id": "pSAFDnZfGOlH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "WHERt1JHK9Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x_data, y_data):\n",
        "    # Normal(0,1) priors on all weights and biases\n",
        "    fc1w_prior = Normal(torch.zeros_like(net.fc1.weight),\n",
        "                        torch.ones_like(net.fc1.weight)).to_event(2)\n",
        "    fc1b_prior = Normal(torch.zeros_like(net.fc1.bias),\n",
        "                        torch.ones_like(net.fc1.bias)).to_event(1)\n",
        "    outw_prior = Normal(torch.zeros_like(net.out.weight),\n",
        "                        torch.ones_like(net.out.weight)).to_event(2)\n",
        "    outb_prior = Normal(torch.zeros_like(net.out.bias),\n",
        "                        torch.ones_like(net.out.bias)).to_event(1)\n",
        "\n",
        "    priors = {\n",
        "        'fc1.weight': fc1w_prior,\n",
        "        'fc1.bias': fc1b_prior,\n",
        "        'out.weight': outw_prior,\n",
        "        'out.bias': outb_prior\n",
        "    }\n",
        "\n",
        "    lifted_model = pyro.random_module(\"module\", net, priors)\n",
        "    lifted_reg_model = lifted_model()  # one sample of network parameters\n",
        "\n",
        "    # Forward pass\n",
        "    logits = lifted_reg_model(x_data)\n",
        "\n",
        "    # Each image is an independent observation\n",
        "    with pyro.plate(\"data\", x_data.size(0)):\n",
        "        pyro.sample(\"obs\", Categorical(logits=logits), obs=y_data)"
      ],
      "metadata": {
        "id": "cfF5uNG2LGc5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide"
      ],
      "metadata": {
        "id": "AV_Aqj7zNJ1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soft_plus = torch.nn.Softplus()\n",
        "\n",
        "def guide(x_data, y_data):\n",
        "    # Layer 1 weight posterior\n",
        "    fc1w_mu = pyro.param(\"fc1w_mu\", torch.randn_like(net.fc1.weight))\n",
        "    fc1w_sigma_param = pyro.param(\"fc1w_sigma\", torch.ones_like(net.fc1.weight))\n",
        "    fc1w_sigma = soft_plus(fc1w_sigma_param)\n",
        "    fc1w_dist = Normal(fc1w_mu, fc1w_sigma).to_event(2)\n",
        "\n",
        "    # Layer 1 bias posterior\n",
        "    fc1b_mu = pyro.param(\"fc1b_mu\", torch.randn_like(net.fc1.bias))\n",
        "    fc1b_sigma_param = pyro.param(\"fc1b_sigma\", torch.ones_like(net.fc1.bias))\n",
        "    fc1b_sigma = soft_plus(fc1b_sigma_param)\n",
        "    fc1b_dist = Normal(fc1b_mu, fc1b_sigma).to_event(1)\n",
        "\n",
        "    # Output layer weight posterior\n",
        "    outw_mu = pyro.param(\"outw_mu\", torch.randn_like(net.out.weight))\n",
        "    outw_sigma_param = pyro.param(\"outw_sigma\", torch.ones_like(net.out.weight))\n",
        "    outw_sigma = soft_plus(outw_sigma_param)\n",
        "    outw_dist = Normal(outw_mu, outw_sigma).to_event(2)\n",
        "\n",
        "    # Output layer bias posterior\n",
        "    outb_mu = pyro.param(\"outb_mu\", torch.randn_like(net.out.bias))\n",
        "    outb_sigma_param = pyro.param(\"outb_sigma\", torch.ones_like(net.out.bias))\n",
        "    outb_sigma = soft_plus(outb_sigma_param)\n",
        "    outb_dist = Normal(outb_mu, outb_sigma).to_event(1)\n",
        "\n",
        "    dists = {\n",
        "        'fc1.weight': fc1w_dist,\n",
        "        'fc1.bias': fc1b_dist,\n",
        "        'out.weight': outw_dist,\n",
        "        'out.bias': outb_dist\n",
        "    }\n",
        "\n",
        "    lifted_module = pyro.random_module(\"module\", net, dists)\n",
        "    return lifted_module()\n"
      ],
      "metadata": {
        "id": "_smNJWr7sK00"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Posterior Samplying"
      ],
      "metadata": {
        "id": "3U37nXyioyXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict(x,K=10):\n",
        "  sample_models=[guide[None,None] for _ in range(K)]\n",
        "  yhats=[]\n",
        "  for sample_nets in sample_models:\n",
        "    logits=sample_nets(x.view(x.size(0), -1))\n",
        "    yhats.append(F.softmax(logits,dim=1))\n",
        "  yhats=torch.stack(yhats)  # [K, batch, num_classes]\n",
        "  mean_probs=yhats.mean(0)\n",
        "  var_probs=yhats.var(1)\n",
        "  return mean_probs,var_probs"
      ],
      "metadata": {
        "id": "n1FOsGsO6D-9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rejection Threshold"
      ],
      "metadata": {
        "id": "TLZKGoy2sH7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_batch(x , y, threshold=0.7):\n",
        "  prob=predict(x)\n",
        "  label=prob.argmax(1)\n",
        "  maxProb=prob.max(1).values\n",
        "\n",
        "  confident=maxProb > threshold\n",
        "  correct=(label[confident]==y[confident]).sum().item()\n",
        "  total=y.size(0)\n",
        "  total_confident=confident.sum().item()\n",
        "\n",
        "  if total_confident >0:\n",
        "    acc=correct/total_confident\n",
        "\n",
        "  refused=1-total_confident/total # proportion without a label\n",
        "\n",
        "  return acc,refused"
      ],
      "metadata": {
        "id": "UTN8wtHesmJp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Uncertainty"
      ],
      "metadata": {
        "id": "IgPH98tTwSD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_uncertainty(x, num_samples=10):\n",
        "    mean_probs, var_probs = predict(x[:num_samples])\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(x[i].permute(1,2,0).numpy())\n",
        "        plt.axis('off')\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.bar(range(mean_probs.shape[1]), mean_probs[i].numpy())\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "NqFYy2W3uv-L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "wiCcpzKYVsvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Categorical\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# ============================================================\n",
        "# 1️⃣ Dataset Definition\n",
        "# ============================================================\n",
        "\n",
        "class CUBDataset(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "\n",
        "        img_files = pd.read_csv(os.path.join(root, \"images.txt\"), sep=\" \", names=[\"img_id\", \"filepath\"])\n",
        "        labels = pd.read_csv(os.path.join(root, \"image_class_labels.txt\"), sep=\" \", names=[\"img_id\", \"target\"])\n",
        "        split = pd.read_csv(os.path.join(root, \"train_test_split.txt\"), sep=\" \", names=[\"img_id\", \"is_training_img\"])\n",
        "        df = img_files.merge(labels, on=\"img_id\").merge(split, on=\"img_id\")\n",
        "        df = df[df[\"is_training_img\"] == int(train)]\n",
        "\n",
        "        self.paths = df[\"filepath\"].values\n",
        "        self.targets = df[\"target\"].values - 1  # convert to 0-based\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, \"images\", self.paths[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.targets[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2️⃣ Simple Neural Network (for Bayesian wrapping)\n",
        "# ============================================================\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=3*128*128, hidden_size=256, output_size=200):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3️⃣ Model and Guide for Bayesian Training\n",
        "# ============================================================\n",
        "\n",
        "soft_plus = nn.Softplus()\n",
        "\n",
        "def model(x_data, y_data):\n",
        "    fc1w_prior = Normal(torch.zeros_like(net.fc1.weight), torch.ones_like(net.fc1.weight)).to_event(2)\n",
        "    fc1b_prior = Normal(torch.zeros_like(net.fc1.bias), torch.ones_like(net.fc1.bias)).to_event(1)\n",
        "    outw_prior = Normal(torch.zeros_like(net.out.weight), torch.ones_like(net.out.weight)).to_event(2)\n",
        "    outb_prior = Normal(torch.zeros_like(net.out.bias), torch.ones_like(net.out.bias)).to_event(1)\n",
        "\n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,\n",
        "              'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "\n",
        "    lifted_net = pyro.random_module(\"module\", net, priors)\n",
        "    sampled_net = lifted_net()\n",
        "\n",
        "    with pyro.plate(\"data\", x_data.size(0)):\n",
        "        logits = sampled_net(x_data)\n",
        "        pyro.sample(\"obs\", Categorical(logits=logits), obs=y_data)\n",
        "\n",
        "\n",
        "def guide(x_data, y_data):\n",
        "    # mean and std for each parameter\n",
        "    fc1w_mu = pyro.param(\"fc1w_mu\", torch.randn_like(net.fc1.weight))\n",
        "    fc1w_sigma = soft_plus(pyro.param(\"fc1w_sigma\", torch.ones_like(net.fc1.weight)))\n",
        "\n",
        "    fc1b_mu = pyro.param(\"fc1b_mu\", torch.randn_like(net.fc1.bias))\n",
        "    fc1b_sigma = soft_plus(pyro.param(\"fc1b_sigma\", torch.ones_like(net.fc1.bias)))\n",
        "\n",
        "    outw_mu = pyro.param(\"outw_mu\", torch.randn_like(net.out.weight))\n",
        "    outw_sigma = soft_plus(pyro.param(\"outw_sigma\", torch.ones_like(net.out.weight)))\n",
        "\n",
        "    outb_mu = pyro.param(\"outb_mu\", torch.randn_like(net.out.bias))\n",
        "    outb_sigma = soft_plus(pyro.param(\"outb_sigma\", torch.ones_like(net.out.bias)))\n",
        "\n",
        "    dists = {\n",
        "        \"fc1.weight\": Normal(fc1w_mu, fc1w_sigma).to_event(2),\n",
        "        \"fc1.bias\": Normal(fc1b_mu, fc1b_sigma).to_event(1),\n",
        "        \"out.weight\": Normal(outw_mu, outw_sigma).to_event(2),\n",
        "        \"out.bias\": Normal(outb_mu, outb_sigma).to_event(1),\n",
        "    }\n",
        "\n",
        "    lifted_net = pyro.random_module(\"module\", net, dists)\n",
        "    return lifted_net()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4️⃣ Prediction and Evaluation\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(x, K=10):\n",
        "    sampled_models = [guide(None, None) for _ in range(K)]\n",
        "    yhats = []\n",
        "    for sampled_net in sampled_models:\n",
        "        logits = sampled_net(x.view(x.size(0), -1))\n",
        "        yhats.append(F.softmax(logits, dim=1))\n",
        "    yhats = torch.stack(yhats)\n",
        "    mean_probs = yhats.mean(0)\n",
        "    var_probs = yhats.var(0)\n",
        "    return mean_probs, var_probs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader, threshold=0.5):\n",
        "    correct, total, confident_total = 0, 0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        mean_probs, _ = predict(imgs)\n",
        "        preds = mean_probs.argmax(1)\n",
        "        conf = mean_probs.max(1).values\n",
        "        confident_mask = conf > threshold\n",
        "        correct += (preds[confident_mask] == labels[confident_mask]).sum().item()\n",
        "        confident_total += confident_mask.sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = correct / confident_total if confident_total > 0 else 0\n",
        "    skip_rate = 1 - confident_total / total\n",
        "    return acc, skip_rate\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5️⃣ Training Loop\n",
        "# ============================================================\n",
        "\n",
        "def train_bnn(num_epochs=5, beta=0.5):\n",
        "    pyro.clear_param_store()\n",
        "    svi = SVI(model, guide, Adam({\"lr\": 1e-3}), loss=Trace_ELBO())\n",
        "\n",
        "    loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # Standard ELBO step\n",
        "            elbo = svi.step(imgs, labels)\n",
        "\n",
        "            # Apply β coefficient manually (optional annealing)\n",
        "            total_loss += beta * elbo\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader.dataset)\n",
        "        loss_history.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | β={beta:.2f} | Avg ELBO loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6️⃣ Main Script\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_root = \"/content/data/CUB_200_2011\"\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_data = CUBDataset(data_root, train=True, transform=transform)\n",
        "    test_data = CUBDataset(data_root, train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"✅ Loaded {len(train_data)} training and {len(test_data)} testing images.\")\n",
        "\n",
        "    # setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net = SimpleNN().to(device)\n",
        "\n",
        "    # === Train ===\n",
        "    loss_hist = train_bnn(num_epochs=50, beta=0.5)\n",
        "\n",
        "    # === Evaluate ===\n",
        "    acc, skip = evaluate(test_loader, threshold=0.05)\n",
        "    print(f\"Accuracy (when predicted): {acc*100:.2f}% | Skip rate: {skip*100:.2f}%\")\n",
        "\n",
        "    # === Plot loss ===\n",
        "    plt.plot(loss_hist)\n",
        "    plt.title(\"ELBO Loss over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "yiRJ2gb1VipW",
        "outputId": "34f16651-a295-4e95-fc2f-fa28e39687c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 5994 training and 5794 testing images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyro/primitives.py:526: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | β=0.50 | Avg ELBO loss: 114977.0239\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3250808776.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# === Train ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mloss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_bnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# === Evaluate ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3250808776.py\u001b[0m in \u001b[0;36mtrain_bnn\u001b[0;34m(num_epochs, beta)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# Standard ELBO step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# Apply β coefficient manually (optional annealing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         params = set(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             ):\n\u001b[1;32m    156\u001b[0m                 \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0msurrogate_loss_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mwarn_if_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}